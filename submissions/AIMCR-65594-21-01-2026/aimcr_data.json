{
  "metadata": {
    "proposal_title": "Scaling Laws and Optimization Dynamics of Looped Transformers: Toward Efficient and High-Capacity Language Models",
    "principal_investigator": "Di Wang",
    "proposal_date": "2026-01-11",
    "reviewer_name": "Mohsin Ahmed Shaikh",
    "reviewer_id": "174988",
    "aimcr_date": "2026-01-11",
    "project_id": "65594"
  },
  "third_party_software": [
    {
      "name": "PyTorch",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Required to implement and run transformer-based LLM training experiments (including distributed training with Megatron-LM) to study Looped Transformer scaling laws and optimization dynamics. Aligns with the approved topic, Data Science and Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the PyTorch 2.9.0 documentation, which describes a general-purpose open-source machine learning framework; this review was conducted in accordance with LC 2.7."
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "No indication of restricted-country origin or restricted-entity involvement was identified for Torchtitan v0.2.0 based on available documentation, contributor information, and repository metadata.\\nThis review was conducted in accordance with LC 2.5."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "3.1 Source Channel\nGitHub: https://github.com/pytorch/pytorch\n3.2 GitHub Repository Metadata\nOwner / Organization: PyTorch Foundation\nRepository: pytorch/pytorch\nDescription: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nStars: 96510\nForks: 26483\nOpen Issues: 17875\nLast Commit Date: 2026-01-11T03:50:11Z\nActivity Health: Healthy"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "4.1 License Type\nSPDX ID: N/A\nLicense name: BSD 3-Clause\n4.2 Permissibility Summary\nQuestion\tAnswer\nCan be used for academic purposes?\tYes\nCan be used for commercial use?\tYes\nModification allowed?\tYes\nRedistribution allowed?\tYes\nAttribution required?\tYes\nCopyleft obligations?\tNo"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "torch 2.9.0\nnvidia-cublas-cu12 12.8.4.1\nnvidia-cuda-cupti-cu12 12.8.90\nnvidia-cuda-nvrtc-cu12 12.8.93\nnvidia-cuda-runtime-cu12 12.8.90\nnvidia-cudnn-cu12 9.10.2.21\nnvidia-cufft-cu12 11.3.3.83\nnvidia-cufile-cu12 1.13.1.3\nnvidia-curand-cu12 10.3.9.90\nnvidia-cusolver-cu12 11.7.3.90\nnvidia-cusparse-cu12 12.5.8.93\nnvidia-cusparselt-cu12 0.7.1\nnvidia-nccl-cu12 2.27.5\nnvidia-nvjitlink-cu12 12.8.93\nnvidia-nvshmem-cu12 3.3.20\nnvidia-nvtx-cu12 12.8.90\ntriton 3.5.0\nfsspec 2026.1.0\nnetworkx 3.6.1\nsympy 1.14.0\nmpmath 1.3.0\ntyping_extensions 4.15.0\nfilelock 3.20.3\nJinja2 3.1.6\nMarkupSafe 3.0.3\nsetuptools 80.9.0\n"
        }
      ]
    },
    {
      "name": "Megatron-LM (0.15.0)",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Required to implement and scale large transformer model pretraining to support the project\u2019s objectives of analyzing and optimizing LLM training dynamics. Aligns with the approved topic, Data Science and Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the Megatron-LM v0.15.0 documentation, which describes a research-oriented large-scale language model training framework; this review was conducted in accordance with LC 2.7."
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "No indication of restricted-country origin or restricted-entity involvement was identified for Torchtitan v0.2.0 based on available documentation, contributor information, and repository metadata. This review was conducted in accordance with LC 2.5."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "3.1 Source Channel\nGitHub: https://github.com/NVIDIA/Megatron-LM\n3.2 GitHub Repository Metadata\nOwner / Organization: NVIDIA\nRepository: NVIDIA/Megatron-LM\nDescription: Ongoing research training transformer models at scale\nStars: 14862\nForks: 3478\nOpen Issues: 558\nLast Commit Date: 2026-01-10T07:07:09Z\nActivity Health: Healthy"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": ".1 License Type\nSPDX ID: N/A\nLicense name: Apache 2.0\n4.2 Permissibility Summary\nQuestion\tAnswer\nCan be used for academic purposes?\tYes\nCan be used for commercial use?\tYes\nModification allowed?\tYes\nRedistribution allowed?\tYes\nAttribution required?\tYes\nCopyleft obligations?\tNo"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "megatron-core 0.15.0\nnumpy 1.26.4\npackaging 25.0\ntorch 2.9.1\nnvidia-cublas-cu12 12.8.4.1\nnvidia-cuda-cupti-cu12 12.8.90\nnvidia-cuda-nvrtc-cu12 12.8.93\nnvidia-cuda-runtime-cu12 12.8.90\nnvidia-cudnn-cu12 9.10.2.21\nnvidia-cufft-cu12 11.3.3.83\nnvidia-cufile-cu12 1.13.1.3\nnvidia-curand-cu12 10.3.9.90\nnvidia-cusolver-cu12 11.7.3.90\nnvidia-cusparse-cu12 12.5.8.93\nnvidia-cusparselt-cu12 0.7.1\nnvidia-nccl-cu12 2.27.5\nnvidia-nvjitlink-cu12 12.8.93\nnvidia-nvshmem-cu12 3.3.20\nnvidia-nvtx-cu12 12.8.90\ntriton 3.5.1\nfsspec 2026.1.0\nnetworkx 3.6.1\nsympy 1.14.0\nmpmath 1.3.0\ntyping_extensions 4.15.0\nfilelock 3.20.3\nJinja2 3.1.6\nMarkupSafe 3.0.3\nsetuptools 80.9.0\n"
        }
      ]
    }
  ],
  "source_code": [],
  "datasets_user_files": [
    {
      "name": "FineWeb-Edu",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "It's the only mentioned dataset."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the FineWeb-Edu dataset documentation, which describes the dataset as an educationally focused web-text corpus for language model research and training; this review was conducted in accordance with LC 2.7."
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "Originating Organization: HuggingFace and Common Crawl both in United States\n"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No files available"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Preview available by HuggingFace\nhttps://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/viewer/default/train?row=0"
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "\nOwner / Curator: HuggingFace / Fine Data\n\nPrimary Distribution Platform: Hugging Face\n\nRepository: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n\nAccess Method: download via Hugging Face\n\nProvenance:\n\nHugging Face does not claim ownership of the original webpages\nPrimary raw source: Common Crawl\nHugging Face does own and license the compiled dataset artifact i.e., the database resulting from selection, transformation, and organization of the data\n(277k downloads on HF, 1 commit, July 2025).\nMaintenance Status: Actively maintained (~20 commits; last update 2025-12)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: ODC-BY\nUse the dataset for any purpose (research, commercial, internal, public)\nModify, transform, and build upon the dataset\nAttribution to the dataset authors is required\nIndicate if changes were made"
        }
      ]
    }
  ],
  "models": [
    {
      "name": "Looped Transformer",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Decoder-only Looped Transformer that reuses several shared Transformer blocks across multiple loop steps (recurrent depth). Trained for autoregressive language modeling to (1) establish scaling laws for loop depth/width/compute and (2) analyze the strong empirical synergy between Looped Transformers and the Muon optimizer. \n\nAligns with approved research topic Datascience & Engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No source or documentation is available to run this check because it doesn't exist at the moment. \nThe model classes are going to come from the libraries that have been successfully screened for prohibited use and no mention of any sensitive terms was found. Based on this, the risk level is set to no-risk therefore it is in compliance with LC2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "No source exists at the moment. \n\nThe model will be developed by Di Wang (PI) and Liangyu Wang (PhD student) on Shaheen III GPUP. Both are working at KAUST. \n\nBoth developers are from China, but are a Faculty member and his student at KAUST. "
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "No license associated. The code will be developed on Shaheen III GPUP. "
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "Pre-training of the models are planned to use the FineWeb-Edu [1] public text corpus (10 B to 350 B tokens). (HuggingFaceFW/fineweb-edu; Lozhkov et al., 2024; DOI: 10.57967/hf/2497; arXiv:2406.17557). "
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Models are trained from scratch for language modeling; no downstream fine-tuning in the released artifacts (unless added in later follow-up work). "
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Indicated number of parameters ~0.1B\u201310B (via loop depth). Considering 10B for the estimation of FLOPs.  \nNumber of tokens in dataset D= 350B tokens in total \nMFU = 38% to 41% \n\nTotal FLOPs\u22486\u00d7N\u00d7D = 6 x 10 x10^9 x 350\u00d710^9 = 2.1x10^22\n\nThis is orders of magnitude lower than the threshold 10^27.\n\n"
        },
        {
          "name": "Sample Inspection",
          "score": 2,
          "notes": "Inspection skipped. No source."
        }
      ],
      "is_proprietary": false
    }
  ],
  "observations": "Model is to be developed on Shaheen III GPUP from scratch. \nThe model source, therefore will need to be checked at least once to confirm the conformance to the project proposal. \n\nSample check could not be carried out because of lack of source for the model.\n\nFor software, the group has not listed transformers library. In this case, it is expected that they will use torch.nn to define the model.\n\n",
  "recommendation": "Positive check passed. \n\nNegative check passed.\n\n10^27 check passed.\n\nFinal recommended for the grand challenge, with a condition to monitor midway to the allocated duration approved by the RCAC.",
  "_submission_history": [
    {
      "timestamp": "2026-01-22T12:21:14.937468",
      "action": "resubmission"
    }
  ]
}