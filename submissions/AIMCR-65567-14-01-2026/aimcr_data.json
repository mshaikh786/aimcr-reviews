{
  "metadata": {
    "proposal_title": "MultiWeave: Cross\u2010Model Collaboration and Orchestration of Multi\u2010Model AI Systems",
    "principal_investigator": "Marco Canini",
    "proposal_date": "2025-12-09",
    "reviewer_name": "Mohsin Ahmed Shaikh",
    "reviewer_id": "174988",
    "aimcr_date": "2026-01-20",
    "project_id": "65567"
  },
  "third_party_software": [
    {
      "name": "PyTorch",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "PyTorch is a deep learning framework in Python for training AI models. The proposed research uses it for coding their MultiWeave collaboration Framework. \nAligns with the approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation of PyTorch."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Originally developed by Meta and now governed by Linux Foundation."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/pytorch/pytorch\n\nStars: 96600\nForks: 26500 \nGithub projects: 12 \nPRs: 1700\nCommits: 98181\nlast commit: Jan 14th, 2026\n\nKSL will provide NGC container for the requested version of PyTorch. nvcr.io/nvidia/pytorch:25.12-py3"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "KSL will provide NGC container for the requested version of PyTorch. nvcr.io/nvidia/pytorch:25.12-py3\nNVIDIA Deep Learning Container License Permissions:\n\nInstall and Use\nDeploy as a Service\nCreate Derived Containers\nOpen Source Development Restrictions:\nNo Reverse Engineering\nNo Standalone Distribution: may not distribute or sublicense the CONTAINER as a stand-alone product\nNo Circumventing Security"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "Singularity"
        }
      ]
    },
    {
      "name": "Transformers",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Hugging Face Transformers is an open-source Python library providing state-of-the-art pretrained models for natural language processing, computer vision, audio, and multimodal tasks for both inference and training.\n\nThe software is used to:\n\nCoordinate cross-model collaboration among multiple large language models\nImplement contextual bandit and baseline routing algorithms for the MultiWeave framework\nProvide the foundation models (LLMs) that will be orchestrated in the routing experiments\nEnable logging and evaluation of model performance\n\nAligns with the approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation. "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: Hugging Face Inc.\nFounders/Maintainers:\n\nCl\u00e9ment Delangue (CEO) - LinkedIn\nJulien Chaumond (CTO) - [LinkedIn profile available]\nThomas Wolf (Chief Science Officer)\n\n\nAffiliation: Private company based in New York, backed by investors including Salesforce, Google, Amazon, NVIDIA, Intel, AMD, and major VC firms\n\nThe entity is from non-D5 country."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/huggingface/transformers\n\nStars: 153,000+\nForks: 31,100+\nCommits: 21,136+\nLast commit: January 2026\n\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0\nPermissions:\n\nCommercial use allowed\nModification and derivative works permitted\nDistribution allowed\nSublicensing permitted\nPatent grant included (contributors grant patent rights)\nPrivate use allowed\n\nRestrictions:\n\nMust include original copyright notice\nMust include copy of license text\nMust document significant modifications made\nMust preserve attribution notices\nCannot use trademark without permission\nPatent license terminates if you sue over patents"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": ""
        }
      ]
    },
    {
      "name": "Transformers",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Hugging Face Transformers is an open-source Python library providing state-of-the-art pretrained models for natural language processing, computer vision, audio, and multimodal tasks for both inference and training.\n\nThe software is used to:\n\nCoordinate cross-model collaboration among multiple large language models\nImplement contextual bandit and baseline routing algorithms for the MultiWeave framework\nProvide the foundation models (LLMs) that will be orchestrated in the routing experiments\nEnable logging and evaluation of model performance\n\nAligns with the approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation. "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: Hugging Face Inc.\nFounders/Maintainers:\n\nCl\u00e9ment Delangue (CEO) - LinkedIn\nJulien Chaumond (CTO) - [LinkedIn profile available]\nThomas Wolf (Chief Science Officer)\n\n\nAffiliation: Private company based in New York, backed by investors including Salesforce, Google, Amazon, NVIDIA, Intel, AMD, and major VC firms\n\nThe entity is from non-D5 country."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/huggingface/transformers\n\nStars: 153,000+\nForks: 31,100+\nCommits: 21,136+\nLast commit: January 2026\n\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0\nPermissions:\n\nCommercial use allowed\nModification and derivative works permitted\nDistribution allowed\nSublicensing permitted\nPatent grant included (contributors grant patent rights)\nPrivate use allowed\n\nRestrictions:\n\nMust include original copyright notice\nMust include copy of license text\nMust document significant modifications made\nMust preserve attribution notices\nCannot use trademark without permission\nPatent license terminates if you sue over patents"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "conda installation: filelock 3.20.3 packaging 25.0 colorama 0.4.6 certifi 2026.1.4 charset-normalizer 3.4.4 idna 3.11 fsspec 2025.10.0 typing_extensions 4.15.0 dill 0.4.0 python_abi 3.11 aiohappyeyeballs 2.6.1 attrs 25.4.0 pysocks 1.7.1 sniffio 1.3.1 hpack 4.1.0 hyperframe 6.1.0 python-tzdata 2025.3 pytz 2025.2 six 1.17.0 tqdm 4.67.1 exceptiongroup 1.3.1 h11 0.16.0 typing-extensions 4.15.0 cpython 3.11.14 h2 4.3.0 python-dateutil 2.9.0.post0 anyio 4.12.1 python-gil 3.11.14 httpcore 1.0.9 _python_abi3_support 1.0 httpx 0.28.1 libcxx 21.1.8 yaml 0.2.5 aws-c-common 0.12.6 libutf8proc 2.11.2 libopentelemetry-cpp-headers 1.21.0 zlib 1.3.1 libbrotlicommon 1.2.0 zstd 1.5.7 nlohmann_json 3.12.0 c-ares 1.34.6 libev 4.33 libedit 3.1.20250104 libiconv 1.18 libevent 2.1.12 llvm-openmp 21.1.8 xxhash 0.8.3 libssh2 1.11.1 propcache 0.3.1 multidict 6.7.0 multiprocess 0.70.18 safetensors 0.7.0 regex 2025.11.3 hf-xet 1.2.1 gflags 2.2.2 libcrc32c 1.1.2 snappy 1.2.2 lz4-c 1.10.0 brotli-python 1.2.0 libabseil 20250512.1 frozenlist 1.7.0 pyyaml 6.0.3 aws-c-compression 0.3.1 aws-checksums 0.2.7 aws-c-sdkutils 0.2.4 aws-c-cal 0.9.13 libbrotlidec 1.2.0 libbrotlienc 1.2.0 backports.zstd 1.3.0 libnghttp2 1.67.0 krb5 1.21.3 libxml2-16 2.15.1 libthrift 0.22.0 _openmp_mutex 4.5 python-xxhash 3.6.0 yarl 1.22.0 glog 0.7.1 libre2-11 2025.11.05 libprotobuf 6.31.1 aws-c-io 0.23.3 libcurl 8.18.0 libxml2 2.15.1 libgcc 15.2.0 re2 2025.11.05 orc 2.2.1 aws-c-http 0.10.7 aws-c-event-stream 0.5.7 prometheus-cpp 1.3.0 azure-core-cpp 1.16.1 libgfortran5 15.2.0 libgrpc 1.73.1 aws-c-mqtt 0.13.3 aws-c-auth 0.9.3 azure-storage-common-cpp 12.11.0 azure-identity-cpp 1.13.2 libgfortran 15.2.0 libopentelemetry-cpp 1.21.0 libgoogle-cloud 2.39.0 aws-c-s3 0.11.3 azure-storage-blobs-cpp 12.15.0 libopenblas 0.3.30 libgoogle-cloud-storage 2.39.0 aws-crt-cpp 0.35.4 azure-storage-files-datalake-cpp 12.13.0 libblas 3.11.0 aws-sdk-cpp 1.11.606 libcblas 3.11.0 liblapack 3.11.0 libarrow 22.0.0 numpy 1.26.4 libarrow-compute 22.0.0 libparquet 22.0.0 pandas 2.3.3 libarrow-acero 22.0.0 pyarrow-core 22.0.0 libarrow-dataset 22.0.0 libarrow-substrait 22.0.0 pyarrow 22.0.0 aiosignal 1.4.0 urllib3 2.6.3 requests 2.32.5 huggingface_hub 0.36.0 aiohttp 3.13.3 tokenizers 0.20.4 datasets 4.4.2"
        }
      ]
    },
    {
      "name": "vLLM  container image",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "The NVIDIA vLLM NGC container is an optimized Docker image containing vLLM, a high-throughput and memory-efficient inference and serving engine for Large Language Models with GPU acceleration.\n\nvLLM is used for high-throughput inference of multiple LLMs on GH200 GPUs as part of the MultiWeave framework's bandit-style routing experiments and model orchestration.\n\nAligns with approved research topic Datascience & engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Originally developed in Sky Computing Lab at UC Berkeley, now community-driven under PyTorch Foundation; co-created and co-led by Woosuk Kwon (LinkedIn: linkedin.com/in/woosuk-kwon-986551262) and Zhuohan Li, with advisor Ion Stoica (UC Berkeley)."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "nvcr.io/nvidia/vllm:25.12.post1-py3\nhttps://github.com/vllm-project/vllm\n\nStars: 63,100+\nForks: 11,300+\nCommits: 11,322+\nLast commit: January 12, 2026\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0\n\nPermissions:\n\nCommercial use allowed\nModification and derivative works permitted\nDistribution allowed\nSublicensing permitted\nPatent grant included (contributors grant patent rights)\nPrivate use allowed\n\nRestrictions: \n\nMust include original copyright notice\nMust include copy of license text\nMust document significant modifications\nMust preserve attribution notices\nCannot use trademarks without permission\nPatent license terminates if patent litigation initiated"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "singularity"
        }
      ]
    }
  ],
  "source_code": [],
  "datasets_user_files": [
    {
      "name": "mmlu_pro",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "A benchmark for large language models' capabilities in multi-task language understanding. Aligns with the approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation"
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: Wenhu Chen (TIGER-Lab, University of Waterloo) (https://www.linkedin.com/in/wenhu-chen-ab59317b), From a Non-D5 country."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No finetuning scripts provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "This dataset contains 12K complex questions and answers across various disciplines. Low risk."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Primary Distribution Platform: Hugging Face Datasets (https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: MIT License\nUse freely for any purpose including academic research, commercial applications, model evaluation, and educational purposes\nCite MMLU-Pro paper in publications using arXiv: 2406.01574 or NeurIPS 2024 proceedings\nShare, modify, and distribute the dataset\nUse for language model benchmarking, evaluation, training validation, and comparative studies\nCreate derivative works and modified versions\nRestrictions:\n\nMust include copyright notice and permission notice in all copies or substantial portions\nDataset provided \"as is\" without warranty of any kind"
        }
      ]
    },
    {
      "name": "medqa",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "A benchmark for large language models' capabilities in multi-task language understanding. Aligns with the approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation"
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: Di Jin (MIT) (https://www.linkedin.com/in/jindi11/en) From a Non-D5 country"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No finetuning scripts provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "This dataset contains multiple-choice OpenQA for solving medical problems, low risk."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Primary Distribution Platform: Hugging Face Datasets (https://huggingface.co/datasets/bigbio/med_qa)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: MIT License\nUse freely for any purpose including academic research, commercial applications, model evaluation, and educational purposes\nCite MMLU-Pro paper in publications using arXiv: 2406.01574 or NeurIPS 2024 proceedings\nShare, modify, and distribute the dataset\nUse for language model benchmarking, evaluation, training validation, and comparative studies\nCreate derivative works and modified versions\nRestrictions:\n\nMust include copyright notice and permission notice in all copies or substantial portions\nDataset provided \"as is\" without warranty of any kind"
        }
      ]
    },
    {
      "name": "medmcqa",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "MedMCQA is a large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions.. Aligns with the approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation"
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: Ankit Pal (Saama AI Research) (https://www.linkedin.com/in/aadityaura/)  From a Non-D5 country."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No finetuning scripts provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Dataset contains Multiple-Choice Question designed to address real-world medical entrance exam questions. Low risk.\n\nMedMCQA has more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Primary Distribution Platform: Hugging Face Datasets (https://huggingface.co/datasets/openlifescienceai/medmcqa)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache 2.0\nUse freely for any purpose including academic research, commercial applications, and educational purposes\nCite MedMCQA paper in publications using conference proceedings or arXiv reference\nShare, modify, and distribute the dataset\nUse for medical question answering model development, language model evaluation, and clinical reasoning research\nCreate derivative works and modified versions\nRestrictions:\n\nMust include copyright notice and license text in all copies or substantial portions\nMust state changes made to the original dataset if modified\nMust provide attribution to original authors (Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu)\nDataset provided \"as is\" without warranty of any kind"
        }
      ]
    },
    {
      "name": "medqa_usml",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Large-scale real-world medical questions benchmark for Large Language Models. Aligns with the approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation"
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: Shuyue Jia (Boston University) (https://www.linkedin.com/in/bruceshuyuejia) From a Non-D5 country."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No finetuning scripts provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Dataset contains Medical questions / answers. Low risk."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Primary Distribution Platform: Hugging Face Datasets (https://huggingface.co/datasets/shuyuej/MedQA-USMLE-Benchmark)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache 2.0\nUse freely for any purpose, including academic research, commercial use, and internal or external deployments\nShare, copy, modify, and distribute the work in source or object form\nCreate derivative works and release them under your own license terms\nUse in software, datasets, models, benchmarks, and downstream products\nReceive a royalty-free patent license from contributors for necessary patent claims\n\nRestrictions:\nMust include the Apache 2.0 license text with all redistributions\nMust clearly state any changes made to the original work when modified\nMust retain existing copyright, patent, and attribution notices\nMust preserve and reproduce the NOTICE file contents if provided\nNo trademark rights are granted (names/logos stay off-limits)\nPatent license terminates if you initiate patent litigation related to the work\nWork is provided \u201cas is\u201d, without warranties or liability"
        }
      ]
    },
    {
      "name": "mmlu",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areasas a benchmark for Large Language Models. Aligns with the approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation"
        },
        {
          "name": "Restricted Entities Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: Center for AI Safety (CAIS) (https://www.linkedin.com/company/center-for-ai-safety?utm_source=chatgpt.com) is a nonprofit organization based in San Francisco, California that conducts research and advocacy to reduce risks associated with artificial intelligence. It was founded by Dan Hendrycks (https://www.linkedin.com/in/dan-hendrycks-04062395)  , Entity is From a Non-D5 country."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No finetuning scripts provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Dataset contains Multiple choice questions from various fields of knowledge / answers. Low risk."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Primary Distribution Platform: Hugging Face Datasets (https://huggingface.co/datasets/cais/mmlu)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: MIT License\nUse freely for any purpose, including academic research, commercial products, and private or internal use\nCopy, modify, merge, publish, distribute, sublicense, and sell the software\nCreate derivative works and integrate into proprietary or open-source projects\nUse in software, datasets, models, tools, and downstream applications\n\nRestrictions:\nMust include the copyright notice and MIT license text in all copies or substantial portions\nSoftware is provided \u201cas is\u201d, without warranties of any kind\nAuthors and copyright holders are not liable for any damages or claims"
        }
      ]
    }
  ],
  "models": [
    {
      "name": "Multiweave (CoMed)",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": " A bandit\u2011based routing for better understanding of when and how multi\u2011model AI systems can be made both more accurate and more computationally efficient.\n\nAligns with the approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "Model was developed internally at KAUST, No documentation available. "
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "No URL as this code will be developed on Shaheen III GPUP."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": " No license as this is an in-house code to be developed by the applicant and the research group."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "This is not a pre-trained model. The training dataset is the same as inspected in the Datasets section."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Finetuning is planned using medical questions datasets same as inspected in the Datasets section."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "The indication of number of parameters for bandit network is 210K. \nThe indication of number of training tokens is 27M. \nThe estimated FLOPs on ShaheenIII GPUP to train a model from scratch is: 6 x N x D=\n6 x  210 x 10^3 x 27 x 10^6 = 3.4 x 10^13 FLOPS\n\nIt is less than the threshold of 10^27."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "The system uses collaborative reasoning (LeaP - Learning from Peer Feedback) across multiple LLMs with contextual bandit algorithms for optimal subset selection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Llama3-Med42-8B",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "an 8 billion-parameter clinical large language model fine-tuned from LLaMA-3 to answer medical questions and assist with healthcare-related information and reasoning.\n\nAligns with the approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the M42 Health AI Team, part of M42 (https://m42.ae/), a global health technology organization based in Abu Dhabi, UAE, Entity from a Non-D5 country.\n\nSource: https://huggingface.co/m42-health/Llama3-Med42-8B"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Licensed under the Llama 3 Community License Agreement governing use of Meta\u2019s LLaMA-3 materials.\n\nUsers may use, reproduce, distribute, and modify the model under license terms.\n\nRedistribution must include the license and attribution requirements per the Meta Community License.\n\nLicense includes adherence to the LLaMA acceptable use policy and may limit scale or commercial use under certain conditions."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "It was instruction-tuned on a medical corpus of ~1 billion tokens drawn from open clinical sources like flashcards, exam Q&A, and dialogues (pretraining data for base LLaMA-3 itself isn\u2019t disclosed publicly)."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Fine-tuned for medical question answering, clinical reasoning, patient record summarization, and general healthcare Q&A tasks."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Model has 8 billion parameters\nFLOPS = 6 x N x D = 6 x 8 x 10^9 x 1 x 10^9 = 4.8 x 10^19"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risks found under LC 2.7, inspection skipped."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "deepseek-math-7b-instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "A 7 billion-parameter instruction-tuned language model designed for solving mathematical problems, proofs, and quantitative reasoning tasks in natural language.\n\nAligns with the approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Owner: (Hangzhou DeepSeek Artificial Intelligence Co., Ltd.) https://huggingface.co/deepseek-ai/collections Founder: Liang Wenfeng (https://www.linkedin.com/in/wenfeng-liang-837841128/) The entity is from China, one of D5 countries.\n\nSource: https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Released under a DeepSeek custom model license with use-based conditions.\n\nThe license lets you use, modify, and distribute the model and derivatives with attribution and compliance.\n\nUse-based restrictions prohibit certain harmful or misuse scenarios (see Attachment A of the license).\n\nOutput generated by the model is not claimed by DeepSeek, but users are responsible for how it\u2019s used."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The DeepSeekMath series was pretrained starting from a base model and further trained on a mathematics-focused corpus of ~120 billion tokens of domain-specific content, leveraged along with broader web and code data (total tokens likely in the hundreds of billions)."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "It was fine-tuned for instruction conditioning to answer math questions and generate step-by-step mathematical reasoning in a conversational/instruct style."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "The indication of number of parameters for bandit network is 7B. \nThe indication of number of training tokens is 120B. \nThe estimated FLOPs on ShaheenIII GPUP to train a model from scratch is: 6 x N x D= 6 x 7x 10^9 x 120 x 10^9 = 5.04 x 10^21 FLOPS\n\nIt is less than the threshold of 10^27."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risks found under LC 2.7, inspection skipped."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen2.5-7B-Instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Qwen2.5 instruct is a general-purpose large language model for text generation, instruction following, long-text generation, structured data analysis, coding, mathematics, and multilingual conversations across 29+ languages.\n\nAligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is published under the Qwen org and owned/maintained by the Qwen Team at Alibaba Cloud (Alibaba Group); no official maintainer LinkedIn profiles are provided in the primary model/repo pages. China: D5-Country\n\nSource: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\n\nDownloads in Dec 2025: 302,719"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Qwen License (custom, based on Tongyi Qianwen License):\nFree to use, reproduce, modify, and distribute for commercial and non-commercial purposes.\nMust include copy of license and attribution notices with distributions. \nModified files must carry prominent modification notices. \nIf commercial use exceeds 100 million monthly active users, must request additional license from Alibaba. \nCannot use model outputs to improve other LLMs (except Qwen derivatives). \nMust comply with export controls and applicable laws. \nYou own derivative works you create. \nNo trademark license granted except for required notices."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "pre-trained from scratch on 18 trillion tokens from proprietary Alibaba datasets covering 29+ languages, with emphasis on knowledge, coding, and mathematics."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "The indication of number of parameters is 7B. \nThe indication of number of training tokens is 18T. \nThe estimated FLOPs = 6 \u00d7 7 \u00d7 10^9 \u00d7 18 \u00d7 10^12 = 7.56\u00d7 10^23 FLOPs.\nIt is less than the threshold of 10^27."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check, skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Mistral-7B-Instruct-v0.2",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "A 7 billion-parameter instruction-tuned generative language model designed to follow natural language instructions and generate text across conversational and completion tasks.\n\nAligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms like \u201cmilitary,\u201d \u201cweapons,\u201d or \u201cnuclear\u201d in the model documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by Mistral AI (https://mistral.ai/about), a French AI company specializing in open and performant large language models. Entity from a Non-D5 country.\n\nSource: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache 2.0\n\nYou are free to use the model for any purpose, including commercial applications under the Apache 2.0 license.\n\nYou may modify and distribute original or derived models.\n\nYou must include copyright notices and the full license text when redistributing.\n\nNo warranty or liability is provided by the licensors (as per Apache 2.0 terms)."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The base Mistral-7B-Instruct was fine-tuned from Mistral-7B-v0.2 using publicly available conversational and instruction datasets (exact corpus not fully documented), and broadly the underlying base models of this family are in the tens-to-hundreds of billions of tokens range typical for open transformer LLMs."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "It was fine-tuned for instruction following and conversational tasks, making it suitable for chat, question answering, and general text generation when prompted with [INST]\u2026[/INST]-formatted instructions."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "The indication of number of parameters for bandit network is 7B. \nThe estimation of number of training tokens is 2T (No official numbers found). \nThe estimated FLOPs on ShaheenIII GPUP to train a model from scratch is: 6 x N x D= 6 x 7x 10^9 x 2x 10^12 = 8.4 x 10^22 FLOPS\nIt is less than the threshold of 10^27."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check, skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "gte-large-en-v1.5",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "An English text embedding model that converts input text into 1024-dimensional vectors for semantic similarity, retrieval, clustering, or classification tasks with support for up to 8192 tokens of context.\n\nAligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms (e.g., \u201cmilitary\u201d, \u201cweapons\u201d, \u201cnuclear\u201d) in the model documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is developed and maintained by the Institute for Intelligent Computing, Alibaba Group, part of the larger Chinese tech company Alibaba. Entity is from a D5 country (China).\n\nSource: https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache-2.0\n\nFree for commercial or research use with nearly unrestricted reuse rights under the Apache License 2.0 (permissive open-source).\n\nYou may modify and distribute the model and derivative works.\n\nRedistributions must include copyright & license text.\n\nPatent rights are granted under the license, but any patent litigation terminates those rights."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "Training included masked language modeling (C4-en), weak-supervised contrastive pre-training, and supervised contrastive fine-tuning on GTE-specific data; the exact token count is not published, but the dataset size listed is ~10.4 billion tokens from allenai/c4 in the Hugging Face dataset viewer."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "It\u2019s pre-trained/contrastively tuned for general text representation (embeddings), optimized for tasks like semantic similarity, text retrieval, clustering, and reranking."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "The indication of number of parameters for bandit network is 0.43B. \nThe indication of number of training tokens is ~10\u201315 B tokens. \nThe estimated FLOPs on ShaheenIII GPUP to train a model from scratch is: 6 x N x D= 6 x 0.43x 10^9 x 15 x 10^9 = 3.9 x 10^19 FLOPS\nIt is less than the threshold of 10^27."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check, skipping inspection."
        }
      ],
      "is_proprietary": false
    }
  ],
  "observations": "Model is developed internally in KAUST.",
  "recommendation": "Recommended for Grand Challenge."
}