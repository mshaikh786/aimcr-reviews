{
  "metadata": {
    "proposal_title": " Scaling Multimodal Foundation Models for Physical Intelligence ",
    "principal_investigator": "Mohamed Elhosieny",
    "proposal_date": "2025-12-09",
    "reviewer_name": "Mohsin Ahmed Shaikh",
    "reviewer_id": "174988",
    "aimcr_date": "2025-12-13",
    "project_id": "65638"
  },
  "third_party_software": [
    {
      "name": "Alibaba ROLL",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Needed for the RL training. Aligns with the approved topic, Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "Alibaba DAMO Academy"
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 4,
          "notes": "Alibaba is an organization from D5 country. \nhttps://alibaba.github.io/ROLL/"
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "Source Channel:\nDistribution channel: GitHub (open-source repository)\nExact URL: https://github.com/alibaba/ROLL\n3.2 GitHub Repository Metadata (as of December 16th, 2025)\n\nMaintainer / Organization: Alibaba DAMO Academy\nLast Commit Date: Today\nNumber of Stars: 2.5K\nNumber of Issues (Open/Closed): 61\nNumber of PRs (Open/Closed): 6\nTotal Commits: 368\nActivity Health: Active\n3.3 Package Release History\nLatest release date: Second week of December 2025\nFrequency of releases: monthly"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License Type\nLicense: Apache 2.0\nLicense URL: https://github.com/alibaba/ROLL/blob/main/LICENSE\n4.2 Permissibility Summary\nAcademic purposes: allowed\nCommercial use: allowed\nModification: allowed "
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 3,
          "notes": "Container image from private registry:\nroll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-25.06-py3-torch280-vllm0102\nTotal: 2074 (UNKNOWN: 0, LOW: 136, MEDIUM: 1915, HIGH: 23, CRITICAL: 0)"
        }
      ]
    },
    {
      "name": "RLinf",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Designed for post training the foundation models increase its performance. Aligned with approved topic, datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No prohibited use indication. It is a model training framework. "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 4,
          "notes": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation (Sep 2025) : https://arxiv.org/html/2509.15965v1\nCorresponding author: yu-wang@tsinghua.edu.cn - Tsinghua University\nFirst author: From Tsinghua University, Bejing Zhongguancun Academy China\nAssociated with universities in D5 country."
        },
        {
          "name": "Source / Provenance",
          "score": 3,
          "notes": "https://github.com/RLinf/RLinf\nDocker image: rlinf/rlinf:agentic-rlinf0.1-torch2.6.0-openvla-openvlaoft-pi0 -- owned by a community user \"rlinf\"\n\nMaintained by: \nChao Yu: zoeyuchao@gmail.com\nYu Wang: yu-wang@tsinghua.edu.cn\nBoth associated with the Universities from D:5 country, China\n\nMaintainer / Organization: RLinf organization on GitHub\nLast Commit Date: December 16, 2025.\nNumber of Stars: 1.7K\nNumber of Issues (Open/Closed): 65\nNumber of PRs (Open/Closed): 36\nTotal Commits: 200\nActivity Health: Active"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache License 2.0\nLicense URL: https://github.com/RLinf/RLinf/blob/main/LICENSE\n"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 2,
          "notes": "Dockerhub image: rlinf/rlinf:agentic-rlinf0.1-torch2.6.0-openvla-openvlaoft-pi0 (ubuntu 22.04)\nTotal: 3979 (UNKNOWN: 0, LOW: 238, MEDIUM: 3656, HIGH: 85, CRITICAL: 0)\n"
        }
      ]
    },
    {
      "name": "DeepSpeed",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Distributed training framework from Microsoft. Aligned with approved research topic, datascience & engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No prohibited use indicated. Its a distributed training framework. "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The framework is developed and maintained by Microsoft"
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/microsoft/DeepSpeed\n\nMaintainer / Organization: Microsoft\nLast Commit Date: 4 days ago\nNumber of Stars: 41K\nNumber of Issues (Open/Closed): 1.1K\nNumber of PRs (Open/Closed): 102\nTotal Commits: 3,004\nActivity Health: Active"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache License 2.0\nLicense URL: https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "Installed using the NGC Pytorch base image:\n\nmy-ngc-pytorch28-stack:latest (ubuntu 24.04)\nTotal: 2169 (UNKNOWN: 0, LOW: 118, MEDIUM: 2026, HIGH: 25, CRITICAL: 0) "
        }
      ]
    },
    {
      "name": "HF Accelerate",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Distributed training framework where Deepspeed can be chosen as the distributed training engine. It abstract and makes it easier to train transformer based models and other types.\nAligns with the approved topic, datascience and engieering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No prohibited use indicated."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": " None. It is maintained by HuggingFace organization."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/huggingface/accelerate\n\nMaintainer / Organization: Hugging Face\nLast Commit Date: Today\nNumber of Stars: 9.4K\nNumber of Issues (Open/Closed): 88\nNumber of PRs (Open/Closed): 11\nTotal Commits: 1,891\nActivity Health: Active\nLatest release date: Last month\nFrequency of releases: monthly\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache License 2.0\nLicense URL: https://github.com/huggingface/accelerate/blob/main/LICENSE\n"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "Installed using the base image PyTorch from NGC\n\nmy-ngc-pytorch28-stack:latest (ubuntu 24.04)\nTotal: 2169 (UNKNOWN: 0, LOW: 118, MEDIUM: 2026, HIGH: 25, CRITICAL: 0)"
        }
      ]
    },
    {
      "name": "PyTorch",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "A deep learning framework. Aligned with approved topic, datascience& engineering, "
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "no prohibited use indicated."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Maintained by Meta"
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/pytorch/pytorch\nKSL will provide NGC image of PyTorch with Flash Attention 2.7 pre-installed. Will negotiate the exact pytorch container image version with the applicant. "
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "NVIDIA End user license. \nPermits the proposed work."
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "my-ngc-pytorch28-stack:latest (ubuntu 24.04)\n============================================\nTotal: 2169 (UNKNOWN: 0, LOW: 118, MEDIUM: 2026, HIGH: 25, CRITICAL: 0)"
        }
      ]
    }
  ],
  "source_code": [],
  "datasets_user_files": [
    {
      "name": "Objaverse",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Required in O1-CompoSE for training 3D object generation models and developing text-to-3D synthesis capabilities."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 2,
          "notes": "800K+ images of 3D objects. Categories include:\nTotal unique categories: 18\narchitecture: 76,802\ncharacters-creatures: 59,299\ncultural-heritage-history: 55,614\nfurniture-home: 51,888\nart-abstract: 50,310\nscience-technology: 46,860\nweapons-military: 35,189\nelectronics-gadgets: 28,860\nnature-plants: 28,319\nanimals-pets: 27,479\ncars-vehicles: 26,765\nplaces-travel: 18,487\npeople: 13,199\nfood-drink: 12,047\nfashion-style: 9,595\nsports-fitness: 4,200\nmusic: 2,845\nnews-politics: 951\n\n\"weapons-military\" was inspected and had pictures of avatars of modern and historic e.g. knives, swords, pistols and guns. "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Published as artifact of the papers and appears to be maintained by  Matt Deitke who works at Meta:\nhttps://www.linkedin.com/in/mattdeitke/ \nhttps://mattdeitke.com/\n- Objaverse: A Universe of Annotated 3D Objects\n- Objaverse: A Universe of Annotated 3D Objects"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 2,
          "notes": "\"weapons-military\" category was inspected and had pictures of avatars of modern and historic e.g. knives, swords, pistols and guns. "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://objaverse.allenai.org/\nPublished on HuggingFace\n\nDownloads from Hubgging Face in the month of Nov 2025: 309,132\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "The use of the dataset as a whole is licensed under the ODC-By v1.0 license. Individual objects in Objaverse are licensed under different licenses.\nCreation of derivative databases is allowed under this license."
        }
      ]
    },
    {
      "name": "RoBoMind",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Required for training generalizable robotic manipulation policies, imitation learning research, and Vision-Language-Action (VLA) model development across multiple robot embodiments.\nTotal size : 815 + 514 + 45 + 1678 \u2248 3052 episodes"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "Movies of robots manipulation tasks common objects e.g. household, industrial etc. "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 3,
          "notes": "Affiliated organization:\nThe Beijing Humanoid Robot Innovation Center was established in November 2023 in Beijing Economic and Technological Development Zone (Beijing Yizhuang). It is the first innovation center in China that focuses on the core technology, product development, and application ecosystem construction of humanoid robots.\nhttps://huggingface.co/X-Humanoid\nhttps://x-humanoid-robomind.github.io/"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "None provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "frames from different time stamps of 17 randomly sampled episodes inspected. Aritfacts in https://github.com/mshaikh786/aimcr-scripts/blob/Elhoseiny-DS/65638-Elhoseiny/Dataset-inspection/robomind/robomind.ipynb and corresponding PDF."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "The dataset is accompanied by the paper:\n\u201cRoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation\u201d  https://arxiv.org/abs/2412.13877\n\nRepository Metadata\nLast commit: ~18 days ago (as of 16/12/25)\nMaintainer: X-Humanoid (organization-owned repository)\nRepository activity\nSource page: https://huggingface.co/datasets/x-humanoid-robomind/RoboMIND\nCommits: 656\nDownloads: Downloads last month 38,252 (as of 16/12/25)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0 :  http://www.apache.org/licenses/LICENSE-2.0\nRequires agreeing to T&Cs. License allow academic research and redistribution under same license conditions. Include a copy of the license with any redistribution, retain copyright and attribution notices and mark any files you modify. "
        }
      ]
    },
    {
      "name": "Open X-Embodiment",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "To used in reinforcement learning part of the RL model (O4). Aligns with Datascience and Engineering. \n69 datasets in total. Applicant did not identify which ones they required for their research.\n67 out of 69 datasets contain real RGB image observations. "
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "According to the paper describing the dataset, it contains a range of household objects, from appliances to food items and utensils. https://arxiv.org/html/2310.08864v4 \nThe dataset contains over 1 million real robot trajectories from 22+ robot embodiments, spanning manipulation tasks like pick-and-place, drawer opening, object rearrangement, and more. The data comes from different labs worldwide, each with their own robot setups."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "All authors of the paper had affiliations in 2023 with organizations from non-D5 countries. \nhttps://arxiv.org/html/2310.08864v4 "
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Sample artifacts were inspected in this notebook: https://github.com/mshaikh786/aimcr-scripts/blob/Elhoseiny-DS/65638-Elhoseiny/Dataset-inspection/x-embodiment/x-embodiment.pdf\nIt included RBG camera images of images manipulating objects in a lab environment. "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Maintainer (dataset distributor)\nOrganization: Google DeepMind\nOfficial GitHub organization: google-deepmind\nPrimary repository: google-deepmind/open_x_embodiment\nStar on Github repo: 1.6k\nVery few contributors and commits."
        },
        {
          "name": "License / Permissions",
          "score": 2,
          "notes": "Licenses stated in the official repository\nSoftware code: Apache License 2.0\nData / other materials: Creative Commons Attribution 4.0 (CC-BY 4.0)\nPermissibility:\nAcademic & research use:  Allowed\nCommercial use:  Allowed (with conditions)\nModification: Allowed\nRedistribution:  Allowed\nObligations\nApache-2.0: Preserve license and copyright notices\nCC-BY-4.0: Attribution required when redistributing or publishing derived work\nImportant nuance\nOpen X-Embodiment is an aggregate benchmark. Developers must check individual licenses associated with each component datasets"
        }
      ]
    },
    {
      "name": "LIBERO",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "LIBERO has been requested as a dataset by the applicant, primarily aligned with Objective O3 (ContextFlow) the in-context robot foundation model. It aligns with the approved topic Datascience and Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "Approximately made up of a collection of 6500 multimodal recording. Inspection of images at different timestamp reveals simulated robots working on different object in a lab environment.  "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The dataset is maintained by the Lifelong Robot Learning group, with contributions affiliated with Google DeepMind."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Inspection can be found in https://github.com/mshaikh786/aimcr-scripts/blob/Elhoseiny-DS/65638-Elhoseiny/Dataset-inspection/libero/libero.pdf"
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Maintainer / Organization:\nThe dataset is maintained by the Lifelong Robot Learning group, with contributions affiliated with Google DeepMind.\n\nHugging Face dataset page:\nhttps://huggingface.co/datasets/physical-intelligence/libero\n\nDownloads (last month): 25,029\nGitHub repository:\nhttps://github.com/Lifelong-Robot-Learning/LIBERO/tree/master\n\nLast commit: ~9 months ago\nMaintainer: Google DeepMind\nStatistics:\nStars: ~1.3k\nCommits: 41\nPull Requests: 9\nIssues: 69"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Hugging Face license: CC-BY-4.0\nGitHub repository license: MIT License\nPermissibility:\n- Permitted for academic and research use\n- Redistribution requires inclusion of the original license\n- Attribution to the original authors is required under CC-BY-4.0"
        }
      ]
    },
    {
      "name": "Llava-onevision-dataset",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Algined with training  Scalable Long Video-Language Understanding (objective 2). Algins with research topic datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 2,
          "notes": "No mention of prohibited use terms in the documentation. Inspection reveals simulated images of objects, statistical graphs and piecharts, geometrical problems and solutions, cartoon images depicting stories and captions, xray, sonograms, and MRI images, etc. \n\nDataset in English and Chinese."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Dataset owned and maintained by LMMs-Lab. It is a non-profit research-oriented organization with a group of passionate researchers, we share the sincere passion for developing multimodal intelligence.\n\nLLaVA-OneVision:\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Inspection notebook available "
        },
        {
          "name": "Provenance",
          "score": 3,
          "notes": "Dataset Card Contact (listed on https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data)\nBo Li: drluodian@gmail.com\nKaichen Zhang"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0"
        }
      ]
    },
    {
      "name": "LLaVA-Video-178K",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Algined with training Scalable Long Video-Language Understanding (objective 2). Algins with research topic datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of prohibited use terms in the documentation."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "Dataset owned and maintained by LMMs-Lab. It is a non-profit research-oriented organization with a group of passionate researchers, we share the sincere passion for developing multimodal intelligence.\nCurated by: Yuanhan Zhang (NTU, Singapore),  Jinming Wu (BUPT China), Wei Li (Bytedance)"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 2,
          "notes": "The dataset mixes multiple upstream sources. Only a subset of videos is physically hosted on Hugging Face. The rest are references to external academic datasets, redistributed as metadata-only pointers due to licensing constraints.\nThe curation of this dataset is most suited on Shaheen III CPUP.  "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "No individual maintainer has been identified. Its published under the same LLMs-Lab github as other datasets. \n\n\nDataset is an artifact release with \nCommits: 499\nRecent commit: October 2024\nHF downloads: 15,283\n"
        },
        {
          "name": "License / Permissions",
          "score": 2,
          "notes": "Released under Apache 2.0\nPermissibility:\n- Academic and research use\n- Model training for non-commercial research\n-  Redistribution of raw videos requires caution\n- Commercial / production use without provenance audit is not recommended\nImportant Licensing Note:\n- The dataset aggregates videos from multiple upstream sources with potentially different licenses.\n- Use of the dataset does not grant blanket rights over all included video content.\n\nTo be used only for research and not for redistribution"
        }
      ]
    },
    {
      "name": "CinePile",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "This dataset is required for Objective O2: Scalable Long Video-Language Understanding (LongVU++)."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of prohibited use terms in the documentation."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Tom Goldstein's Lab at University of Maryland, College Park (tomg-group-umd) - Non D5 entity"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "None provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Skipping because risk level is less than 3."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://huggingface.co/datasets/tomg-group-umd/cinepile\nlast commit: 1 year ago\ncommits: 23\nAccess method: Direct download via Hugging Face datasets library\nHF Downloads: 176"
        },
        {
          "name": "License / Permissions",
          "score": 2,
          "notes": "Creative Commons Attribution Non Commercial Share Alike 4.0 International: https://spdx.org/licenses/CC-BY-NC-SA-4.0\nPermitted use:\n- Academic research\n- Model training & evaluation\nRestrictions:\n- Subject to non-commercial or attribution requirements\n- Underlying movie content may carry additional constraints"
        }
      ]
    },
    {
      "name": "InfiniBench",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Relevance to O2 (LongVU++)"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of prohibited use terms in the documentation. The dataset consists of questions derived from TV shows (Castle, Friends, House, Big Bang Theory, etc.)"
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "PI's own dataset created by the group \nnon-D5 entity"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Skipping because risk level is less than 3."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://huggingface.co/datasets/Vision-CAIR/InfiniBench\nOwned by Vision-CAIR \u2014 the same KAUST research group led by Mohamed Elhoseiny (the PI of this proposal). This is an internally developed dataset from the proposing team."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: BSD 3-Clause\nPermissions :\n- Commercial use \u2014 May be used for commercial purposes\n- Modification \u2014 May be modified\n- Distribution \u2014 May be distributed\n- Private use \u2014 May be used privately\nConditions :\nLicense and copyright notice \u2014 A copy of the license and copyright notice must be included with the software\nNo trademark use \u2014 The license does not grant trademark rights\nRestrictions:\n- No liability \n- No warranty\nImportant Note on Underlying Content:\nWhile the InfiniBench annotations and code are BSD 3-Clause licensed, the underlying video content (TV shows like Castle, Friends, House, Big Bang Theory, and movies) remains copyrighted by their respective studios. The dataset provides:\n- Question-answer annotations\n- Paths/references to video files\n- Temporal grounding information\nUsers must obtain the original video content separately through legitimate means (e.g., TVQA dataset, MovieNet dataset), as the copyright for TV shows and movies belongs to the original content owners."
        }
      ]
    },
    {
      "name": "VideoChat-Flash-Training-Data",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "algins with use in O2: Scalable Long Video-Language Understanding (LongVU++). Aligns with approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of prohibited use terms in the documentation. The dataset consists of video sinppet with annotations for multimodal model training."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "Owned by Shanghai AI Laboratory : https://www.shlab.org.cn/\nIt is an entity from D5 country"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "On the landing page of the datasets in HuggingFace, videos are playable for inspection. These are random activities from daily life, sports tutorials, etc. No sensitive sample was witnessed."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "90 commits, last commit was on Jun 24, 2025.\n48125 downloads and 17500 followers as of Jan 2, 2025\nThe dataset is an artifact of a paper : \"https://arxiv.org/html/2501.00574v4\" with authors from Chinese institutions. "
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Released under Apache 2.0\nPermissions:\n- Commercial use -- May be used for commercial purposes\n- Modification -- May be modified freely\n- Distribution -- May be distributed\n- Patent use -- Provides an express grant of patent rights from contributors\n- Private use -- May be used privately\n- Sublicensing -- May grant sublicenses to others\n\nLimitations:\n- No trademark use \u2014 Does not grant trademark rights\n- No liability \u2014 Authors are not liable for damages\n- No warranty \u2014 Provided \"as is\" without warranty"
        }
      ]
    },
    {
      "name": "MovieChat-1K",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "algins with use in O2: Scalable Long Video-Language Understanding (LongVU++). Aligns with approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "The dataset consists of annotated long videos of movie clips with questions and answers. The listed genres are all benign. No detailed documentation of the dataset is available."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The dataset is an artifact of the paper https://arxiv.org/abs/2307.16449. \nAuthors are from a mix of Chinese and US universities and Microsoft Research Asia. Project lead is from U Washington.\n"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Samples inspected on the dataset card in HF. 3 MP4 samples were downloaded and checked, they were all parts of movies. The annotation also did not have prohibited terms."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://huggingface.co/datasets/Enxin/MovieChat-1K_train\nhttps://github.com/rese1f/MovieChat\nThe dataset is artifact of a paper and maintained in two places, GitHub and HF. The following provenance is from HF.\nAs of Jan 3, 2025\n- 1887 commits in total\n- latest commit in May 2024\n- 9333 downloads\n- sole contributor/maintainer: https://huggingface.co/Enxin , a masters student from Zhejiang University https://enxinsong.com/"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "The license is mentioned in Github repo of the dataset. The dataset is released under BSD3-clause. The HF doesn't have a license attached which is a risk.  In general, fundamental research is permissible under this license. "
        }
      ]
    },
    {
      "name": "Falcon-RefineWeb",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 3,
          "notes": "It is not clear which objective of the proposal the dataset aligns with. In general it is aligned with the approved research topic,  Datascience and engineering.\nRefinedWeb is multimodal-friendly. It contains links and alt texts for images in processed samples."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Owned by Technology Innovation Institute which is a government-backed research center in Abu Dhabi, UAE."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Inspected on HF page. Text description with source URLs and corresponding images with URLs directed to them constitute the dataset "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Based on CommonCrawl this is an artifact of the paper: https://arxiv.org/abs/2306.01116\nContact information provided on the dataset card: falconllm@tii.ae\nDownloads as of Jan 3, 2026: 55483\nMultiple commits were squashed into 1 commit in June 2023 which is the latest commit.\n\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": " Open Data Commons License\nPermission to use in fundamental research. "
        }
      ]
    }
  ],
  "models": [
    {
      "name": "TripoSG ",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "TripoSG is a Diffusion Transformer (DiT) model used for 3D shape generation (serving as the pre-trained initializer for the project\u2019s 3D asset generation pipeline). The proposal uses TripoSG to initialize the 3D asset generation component under O1 (simulation-ready 3D content generation). In the additional information, model name mentioned is CoMPoSE"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7. "
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "URL: https://huggingface.co/VAST-AI/TripoSG - 692 Downloads Last Month\n\nThe model developers/maintainers are listed as VAST-AI (base model artifact: TripoSG on Hugging Face), based in USA.\nNo D5+M affiliation found."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "The model is listed as MIT License. \n\nPermits commercial use, modification, and redistribution.\n\nRequires preserving copyright/license notices in redistributed copies.\n\nProvided \u201cas is\u201d (warranty/liability limitations apply)."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "Pretraining data is Objaverse and Objaverse-XL, and the provided dataset-size proxy for compute estimation is ~3M samples in FP32."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Compositional 3D Shape Generation Fine-tuned on a custom dataset extracted from Objaverse. No mention of fine-tuning related to prohibited domains."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical training FLOPs (6\u00b7N\u00b7D)\nTheoretical FLOPs = 6 \u00d7 1.5\u00d710\u2079 \u00d7 3\u00d710\u2076 = 2.7\u00d710\u00b9\u2076 FLOPs.\nWe can treat each sample as T=2048 tokens (as used in the O1 DiT scaling discussion), then D \u2248 3M\u00d72048 = 6.144\u00d710\u2079 tokens and 6\u00b7N\u00b7D \u2248 5.53\u00d710\u00b9\u2079 FLOPs.\n\nDelivered FLOPs (GPU-hours \u00d7 peak \u00d7 MFU) (assuming a conservative MFU of 0.3)\n= 80,640 * 312 * 0.3 = 9.0574848\u00d710^22\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 and 2.5 checks. Skipping inspection. "
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Siglip2",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "SigLIP2 is a pretrained vision(-language) encoder used to turn images/videos into embeddings that can be projected into the LLM space for multimodal training.  \nIt is used in (O2) Scalable Long Video-Language Understanding as the visual encoder backbone for image/video inputs in the multimodal long-context pipeline. \n"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7. "
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "HF URL: https://huggingface.co/google/siglip2-so400m-patch14-384\n285K Downloads last month\nOwned by Google\nNo D5+M affiliation found."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0 license is indicated for SigLIP2 in the provided documentation. "
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The documentation cites WebLI pretraining (10B images + 12B alt-texts, 109 languages) with an estimated ~14.5T token-equivalents usable as D. "
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "The provided documentation states no downstream fine-tuning was done for the released artifact (they use the pretrained model directly). "
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical training FLOPs (6\u00b7N\u00b7D)\nTheoretical FLOPs = 6 \u00b7 (4.0\u00d710^8) \u00b7 (1.45\u00d710^13) = 3.48\u00d710^22 FLOPs\n\nDelivered FLOPs (GPU-hours \u00d7 peak \u00d7 MFU)\nSigLIP2 models were trained on up to 2048 TPUv5e chips, with batch size 32k and 40B examples.\nTPUv5e peak (bf16) per chip = 197 TFLOPs.\nEpoch AI\u2019s models database lists SigLIP 2 training compute \u2248 8.2\u00d710\u00b2\u00b2 FLOPs.\n\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 and 2.5 checks. Skipping inspection. "
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Pi0",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Pi0 (Pi-Zero) is a Vision-Language-Action (VLA) model (PaliGemma backbone + an action expert trained with flow matching) that maps vision + instructions to robot actions for embodied/robot learning experiments. \nPi0/openpi is used for Objective (O3) \u201cContextFlow: In-context robot foundation model\u201d (pre-training + imitation fine-tuning / embodied AI)."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7. "
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "GitHub URL: https://github.com/Physical-Intelligence/openpi \n10K stars - 190 Commits - Last updated one month ago\nOwned By: Physical Intelligence \nLinkedIn: https://www.linkedin.com/company/physical-intelligence\nUSA\nNo D5+M affiliation found"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0.\n\nYou may use, modify, and redistribute the code (source/binary).\n\nYou must include the license and preserve required copyright/NOTICE attributions.\n\nYou must state significant changes you made (when distributing).\n\nThe license includes a patent grant (with termination conditions in case of patent litigation).\n\nNo trademark rights are granted beyond descriptive use."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The openpi base checkpoints are described as pre-trained on \u201c10k+ hours of robot data,\u201d while the proposal\u2019s O3 training plan uses Open-X (~1,000,000 trajectories) plus a similarly sized simulation dataset and samples ~1.95M trajectories for compute estimation (we can use this as D in \u201ctoken-equivalents\u201d if you treat 1 trajectory \u2248 1 unit)."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "The repository provides \u201cexpert\u201d fine-tuned checkpoints for specific robot platforms/tasks (the docs explicitly mention expert checkpoints and training instructions for DROID, and reference platforms like ALOHA)."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "3.3B parameters (\u22483B VLM + \u2248300M action expert). \nFor Objective O3 (ContextFlow), the proposal allocates ~100K GH200 GPU-hours total (\u224897.5K pretraining + \u22482.5K fine-tuning). \n\nTheoretical training FLOPs (6\u00b7N\u00b7D)\nTheoretical FLOPs = 6\u00d73.3\u00d710^9\u00d750\u00d710^9=9.9\u00d710^20 FLOPs\n\nDelivered FLOPs (GPU-hours \u00d7 peak \u00d7 MFU)\nDelivered FLOPs\u2248100,000\u00d73600\u00d7(1979\u00d710^12)\u00d70.40\n=2.84976\u00d710^23 FLOPs (these are the projected on Shaheen III)\nthe \u03c00 paper does not publicly report the training hardware type/count or wall-clock training time\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 and 2.5 checks. Skipping inspection. "
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen3-VL-8B-Instruct ",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Qwen3-VL-8B-Instruct is a vision-language model that takes images/video + text and generates text responses, supporting long-context (native 256K, expandable) multimodal understanding and agentic visual interaction.\nIt best aligns with [O2] Scalable Long Video-Language Understanding, which targets large-scale training for multimodal long-video modeling and long-context scaling via sequence-parallel methods. \nIt is listed in additional information under the name \"Think With Video\""
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is maintained by the Qwen Team (Alibaba Cloud / Alibaba Group)  \nCountry of origin: China\nD5-Country affiliation found."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 licensed (per model card and repo).\n- In the additional info, MIT license is mentioned.\n\nPermits commercial use, modification, and redistribution with required license/NOTICE preservation and attribution.\n\nProvides an express patent license from contributors; terminates for certain patent claims."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The Qwen3 technical report states pretraining uses ~36 trillion tokens (used here as the token-equivalent D estimate, noting Qwen3-VL is multimodal so the true multimodal \u201ctoken-equivalent\u201d may differ)."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Video question answering Instruct finetuning LLaVA-Video-178K "
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical compute: F \u2248 6 \u00b7 N \u00b7 D\n\nN = 9B params\n\nD \u2248 36T tokens\n\nF \u2248 6 \u00d7 9e9 \u00d7 36e12 = 1.944 \u00d7 10\u00b2\u2074 FLOPs\n\nDelivered FLOPs:\nAfter checking literature, It does not report GPU hours, GPU cluster size, or training duration for any specific model like Qwen3-VL-8B.\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": ""
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-7B",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Qwen 2.5 7B is a large language model designed for natural language understanding, text generation, coding, mathematics, multilingual support."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\n\nCountry of origin: China D5-Country affiliation found.\n\nHuggingFace: https://huggingface.co/Qwen/Qwen2.5-7B\nPaper: https://arxiv.org/abs/2407.10671"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\n\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially\nUsers must provide a copy of the Apache 2.0 license with distributions\nUsers must document any modifications made\nUsers may add their own copyright statements to modifications\nPatent licenses terminate if patent litigation is filed against the Work\nNo trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical compute: F \u2248 6 \u00b7 N \u00b7 D\nN = 7B params\n\nD \u2248 18T tokens\n\nF= 6*7*10^9*18*10^12 = 7.56 * 10^23"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-7B-Instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": " Qwen2.5 instruct is a general-purpose large language model for text generation, instruction following, long-text generation, structured data analysis, coding, mathematics, and multilingual conversations across 29+ languages.\nAligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\n\nCountry of origin: China D5-Country affiliation found.\n\nHuggingFace: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Qwen License (custom, based on Tongyi Qianwen License):\nFree to use, reproduce, modify, and distribute for commercial and non-commercial purposes \nMust include copy of license and attribution notices with distributions \nModified files must carry prominent modification notices If commercial use exceeds 100 million monthly active users, must request additional license from Alibaba \nCannot use model outputs to improve other LLMs (except Qwen derivatives) \nMust comply with export controls and applicable laws \nYou own derivative works you create \nNo trademark license granted except for required notices"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical compute: F \u2248 6 \u00b7 N \u00b7 D\nN = 7B params\n\nD \u2248 18T tokens\n\nF= 6*7*10^9*18*10^12 = 7.56 * 10^23"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-14B",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Large language model designed for natural language understanding, text generation, coding, mathematics, multilingual support."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-14B\nPaper: https://arxiv.org/abs/2407.10671"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 14B params\nD \u2248 18T tokens\n\nF= 6 \u00b714 \u00b710^9 \u00b718 \u00b710^12 = 1.512\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-14B-Instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": " Qwen2.5 instruct is a general-purpose large language model for text generation, instruction following, long-text generation, structured data analysis, coding, mathematics, and multilingual conversations across 29+ languages. Aligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-14B-Instruct"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 14B params\nD \u2248 18T tokens\n\nF= 6 \u00b714 \u00b710^9 \u00b718 \u00b710^12 = 1.512\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-32B",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": " Large language model designed for natural language understanding, text generation, coding, mathematics, multilingual support."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-32B"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 32B params\nD \u2248 18T tokens\n\nF= 6 \u00b732 \u00b710^9 \u00b718 \u00b710^12 = 3.456\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-32B-Instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Qwen2.5 instruct is a general-purpose large language model for text generation, instruction following, long-text generation, structured data analysis, coding, mathematics, and multilingual conversations across 29+ languages. Aligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 32B params\nD \u2248 18T tokens\n\nF= 6 \u00b732 \u00b710^9 \u00b718 \u00b710^12 = 3.456\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    }
  ],
  "observations": "- For the TripoSG model, it is listed in the additional info under the name \"CoMPoSE\"\n- \nBoth ROLL and RLinf have maintainers from D5 countries. \nRoboMIND is also a D5 country's opensource contribution. ",
  "recommendation": "Recommended for grand challenge conditional to acceptable risk with models. "
}