{
  "metadata": {
    "proposal_title": " Scaling Multimodal Foundation Models for Physical Intelligence ",
    "principal_investigator": "Mohamed Elhosieny",
    "proposal_date": "2025-12-09",
    "reviewer_name": "Mohsin Ahmed Shaikh",
    "reviewer_id": "174988",
    "aimcr_date": "2025-12-13",
    "project_id": "65638"
  },
  "third_party_software": [
    {
      "name": "Alibaba ROLL",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "ROLL (Reinforcement Learning with Online Learning) is a framework for training Large Language Models and Vision Language Models using reinforcement learning techniques, specifically implementing policy gradient methods like PPO and GRPO.\n\nThe software is used for Objective O4 (EfficientRL-Embodied) - RL post-training for complex tasks and robustness, specifically for implementing policy gradient methods (PPO/GRPO) in combination with DeepSpeed and Hugging Face Accelerate.\n\nAligns with the approved topic, Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "Alibaba is an organization from D5 country. \nhttps://alibaba.github.io/ROLL/"
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "Source Channel:\nExact URL: https://github.com/alibaba/ROLL\n\nOwner: Alibaba Group\n\nOrganization: Alibaba Open Source (https://github.com/alibaba)\n\nPrimary maintainers: The repository is maintained by Alibaba's \nresearch and engineering teams (specific individual LinkedIn profiles not publicly listed in the repository)\n\nStars: 2.5k (2,500)\nForks: 177\nCommits: 367\nLast commit: December 2025 (actively maintained - latest release v0.1.3 on Dec 8, 2025)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache 2.0\nPermissions:\n\nCommercial use allowed\n\nModification allowed\n\nDistribution allowed\n\nPatent use granted\n\nPrivate use allowed"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "The repository lists Dockerfiles to create images using NVIDIA NGC base image. Some popular dependencies like flash attention 2.6.3 numpy, spacy, weasel  etc are being pulled from mirror.aliyun.com. \nThe fix is to pull them from main pypi.org which is more trusted.\n\nDependencies:\nBase image: nvcr.io/nvidia/pytorch:25.06-py3\nsetuptools \nsetuptools_scm \nwheel\ntorch==2.8.0 \ntorchvision==0.23.0 \ntorchaudio==2.8.0\nopencv-python-headless==4.11.0.86\nmegatron-core>=0.13.0,<0.14.0\ndeepspeed==0.16.4\nflash-linear-attention\nvllm==0.11.0\n\n"
        }
      ]
    },
    {
      "name": "RLinf",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "RLinf is a flexible and scalable open-source reinforcement learning infrastructure designed for Embodied and Agentic AI, supporting various RL algorithms (PPO, GRPO, SAC, etc.) and providing high-throughput distributed training capabilities for vision-language-action models and reasoning agents.\n\nThe software is used for Objective O3 (ContextFlow) - In-context robot foundation model for pretraining and imitation fine-tuning using reinforcement learning on robot datasets, as explicitly mentioned in the proposal's codes & libraries section.\n\nAligned with the approved topic, datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "Owner: RLinf Organization (open-source collaborative project)\nPrimary maintainers:\n\nChao Yu (zoeyuchao@gmail.com) - Tsinghua University affiliation\nYu Wang (yu-wang@tsinghua.edu.cn) - Tsinghua University affiliation\n\nMaintainers are from D5 country."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/RLinf/RLinf\n\nStars: 2.1k (2,100)\nForks: 212\nCommits: 254\nLast commit: December 2025 (recently active - v0.1 released on Dec 17, 2025)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache License 2.0\n\nPermissions:\nCommercial use allowed\nModification allowed\nDistribution allowed\nPatent use granted\nPrivate use allowed\n"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "The repository lists Dockerfile to create images using NVIDIA NGC base image. Some dependencies are pulled from  https://mirrors.ustc.edu.cn/. The fix is to pull them from main pypi.org, which is more trusted.\n\nThe following Dockerfile will be used to create a container image for the applicants by removing the references to any unknown mirrors:\n\nhttps://github.com/RLinf/RLinf/blob/main/docker/Dockerfile "
        }
      ]
    },
    {
      "name": "DeepSpeed",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Distributed training framework from Microsoft. Aligned with approved research topic, datascience & engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No prohibited use indicated. Its a distributed training framework. "
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The framework is developed and maintained by Microsoft"
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/microsoft/DeepSpeed\n\nMaintainer / Organization: Microsoft\nLast Commit Date: 4 days ago\nNumber of Stars: 41K\nNumber of Issues (Open/Closed): 1.1K\nNumber of PRs (Open/Closed): 102\nTotal Commits: 3,004\nActivity Health: Active"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache License 2.0\nLicense URL: https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "Installed using the NGC Pytorch base image:\n\nmy-ngc-pytorch28-stack:latest (ubuntu 24.04)\nTotal: 2169 (UNKNOWN: 0, LOW: 118, MEDIUM: 2026, HIGH: 25, CRITICAL: 0) "
        }
      ]
    },
    {
      "name": "HF Accelerate",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Distributed training framework where Deepspeed can be chosen as the distributed training engine. It abstract and makes it easier to train transformer based models and other types.\nAligns with the approved topic, datascience and engieering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No prohibited use indicated."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": " None. It is maintained by HuggingFace organization."
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/huggingface/accelerate\n\nMaintainer / Organization: Hugging Face\nLast Commit Date: Today\nNumber of Stars: 9.4K\nNumber of Issues (Open/Closed): 88\nNumber of PRs (Open/Closed): 11\nTotal Commits: 1,891\nActivity Health: Active\nLatest release date: Last month\nFrequency of releases: monthly\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: Apache License 2.0\nLicense URL: https://github.com/huggingface/accelerate/blob/main/LICENSE\n"
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "Installed using the base image PyTorch from NGC\n\nmy-ngc-pytorch28-stack:latest (ubuntu 24.04)\nTotal: 2169 (UNKNOWN: 0, LOW: 118, MEDIUM: 2026, HIGH: 25, CRITICAL: 0)"
        }
      ]
    },
    {
      "name": "PyTorch",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "A deep learning framework. Aligned with approved topic, datascience& engineering, "
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "no prohibited use indicated."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Maintained by Meta"
        },
        {
          "name": "Source / Provenance",
          "score": 1,
          "notes": "https://github.com/pytorch/pytorch\nKSL will provide NGC image of PyTorch with Flash Attention 2.7 pre-installed. Will negotiate the exact pytorch container image version with the applicant. "
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "NVIDIA End user license. \nPermits the proposed work."
        },
        {
          "name": "Bundled Tools / Dependencies",
          "score": 1,
          "notes": "my-ngc-pytorch28-stack:latest (ubuntu 24.04)\n============================================\nTotal: 2169 (UNKNOWN: 0, LOW: 118, MEDIUM: 2026, HIGH: 25, CRITICAL: 0)"
        }
      ]
    }
  ],
  "source_code": [],
  "datasets_user_files": [
    {
      "name": "Objaverse",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Required in O1-CompoSE for training 3D object generation models and developing text-to-3D synthesis capabilities."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "800K+ images of 3D objects. Categories include:\nTotal unique categories: 18\narchitecture: 76,802\ncharacters-creatures: 59,299\ncultural-heritage-history: 55,614\nfurniture-home: 51,888\nart-abstract: 50,310\nscience-technology: 46,860\nweapons-military: 35,189\nelectronics-gadgets: 28,860\nnature-plants: 28,319\nanimals-pets: 27,479\ncars-vehicles: 26,765\nplaces-travel: 18,487\npeople: 13,199\nfood-drink: 12,047\nfashion-style: 9,595\nsports-fitness: 4,200\nmusic: 2,845\nnews-politics: 951\n\n\"weapons-military\" was inspected and had pictures of avatars of modern and historic e.g. knives, swords, pistols and guns. "
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Published as artifact of the papers and appears to be maintained by  Matt Deitke who works at Meta:\nhttps://www.linkedin.com/in/mattdeitke/ \nhttps://mattdeitke.com/\n- Objaverse: A Universe of Annotated 3D Objects\n- Objaverse: A Universe of Annotated 3D Objects"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "\"weapons-military\" category was inspected and had pictures of avatars of modern and historic e.g. knives, swords, pistols and guns. "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://objaverse.allenai.org/\nPublished on HuggingFace\n\nDownloads from Hubgging Face in the month of Nov 2025: 309,132\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "The use of the dataset as a whole is licensed under the ODC-By v1.0 license. Individual objects in Objaverse are licensed under different licenses.\nCreation of derivative databases is allowed under this license."
        }
      ]
    },
    {
      "name": "RoBoMind",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "The dataset contains 107K real-world robot demonstration trajectories across 479 tasks involving 96 object classes, with multi-view RGB-D observations, proprioceptive robot state information, end effector details, linguistic task descriptions, and 5K failure demonstrations from four robotic embodiments (Franka Panda, UR5e, AgileX dual-arm, Tien Kung humanoid robot).\n\nThe dataset contains 107K real-world robot demonstration trajectories across 479 tasks involving 96 object classes, with multi-view RGB-D observations, proprioceptive robot state information, end effector details, linguistic task descriptions, and 5K failure demonstrations from four robotic embodiments (Franka Panda, UR5e, AgileX dual-arm, Tien Kung humanoid robot).\n\nRequired for training generalizable robotic manipulation policies, imitation learning research, and Vision-Language-Action (VLA) model development across multiple robot embodiments.\nTotal size : 815 + 514 + 45 + 1678 \u2248 3052 episodes"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "Primary Owner: Beijing Innovation Center of Humanoid Robotics (X-Humanoid), also known as National and Local Co-built Embodied AI Robotics Innovation Center\n\nCo-creators: State Key Laboratory of Multimedia Information Processing, Peking University; Beijing Academy of Artificial Intelligence\n\nMaintainers:\n\nKun Wu (Beijing Innovation Center of Humanoid Robotics), \nChengkai Hou (Peking University), \nJiaming Liu (Peking University), \nZhengping Che (Beijing Innovation Center of Humanoid Robotics, project leader), \nXiaozhu Ju (Beijing Innovation Center of Humanoid Robotics, project leader), \nShanghang Zhang (Peking University, Beijing Academy of Artificial Intelligence), \nJian Tang (Beijing Innovation Center of Humanoid Robotics, Chief Technology Officer)"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "None provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Frames from different time stamps of 17 randomly sampled episodes were inspected.\n\nAritfacts in https://github.com/mshaikh786/aimcr-scripts/blob/master/65638-Elhoseiny/Dataset-inspection/robomind/robomind.ipynb and corresponding PDF.\n\nRobotic arms are manipulating household items. "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "The dataset is accompanied by the paper:\n\u201cRoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation\u201d  https://arxiv.org/abs/2412.13877\n\nRepository Metadata\nLast commit: ~18 days ago (as of 16/12/25)\nMaintainer: X-Humanoid (organization-owned repository)\nRepository activity\nSource page: https://huggingface.co/datasets/x-humanoid-robomind/RoboMIND\nCommits: 656\nDownloads: Downloads last month 38,252 (as of 16/12/25)"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0 :  http://www.apache.org/licenses/LICENSE-2.0\nRequires agreeing to T&Cs. License allow academic research and redistribution under same license conditions. Include a copy of the license with any redistribution, retain copyright and attribution notices and mark any files you modify. "
        }
      ]
    },
    {
      "name": "Open X-Embodiment",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "The dataset contains 1M+ real robot demonstration trajectories spanning 22 robot embodiments (single arms, bi-manual robots, quadrupeds) from 60 existing datasets pooled from 34 robotic research labs worldwide, demonstrating 527 manipulation skills across 160,266 tasks, with RGB images, language instructions, proprioceptive state information, and 7-dimensional end-effector actions standardized in RLDS format.\n\nThe dataset is used for Objective O3 (ContextFlow) - In-context robot foundation model for pretraining on large-scale multi-embodiment robot datasets (Open-X with 1M trajectories) combined with simulation data before fine-tuning on real-world tasks, as explicitly mentioned in the proposal's methodology section.\n\nAligns with the approved research topic of Datascience and Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Maintainer is Google DeepMind\nEntity is from a non-D5 countries. \n"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Sample artifacts were inspected in this notebook: https://github.com/mshaikh786/aimcr-scripts/blob/master/65638-Elhoseiny/Dataset-inspection/x-embodiment/x-embodiment.ipynb \n\nIt included RBG camera images of images manipulating objects in a lab environment. "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "The data source lists a large collection of datasets. All of the elements are related videos of robotic manipulation of items of daily use. If the project is approved, the KSL will work with the applicant to identify which datasets from the collection are required on Shaheen III GPUP.\n\nhttps://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0\n\nPaper: https://arxiv.org/html/2310.08864v4 \n\nPrimary owner: Google DeepMind Technologies Limited\nCollaborative effort: Open X-Embodiment Collaboration comprising 21 institutions from around the world including: Google DeepMind, Stanford University, UC Berkeley, Carnegie Mellon University, Columbia University, ETH Zurich, EPFL, NYU, UIUC, Arizona State University, Caltech, Allen Institute for AI, and others\nLead contributors: 293+ co-authors including Karl Pertsch, Abby O'Neill, Abdul Rehman, Chelsea Finn, Sergey Levine, Brian Ichter, and many others from participating institutions\nMaintenance: Collaborative maintenance through github.com/google-deepmind/open_x_embodiment\nContact: open-x-embodiment@googlegroups.com"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Licenses stated in the official repository\nSoftware code: Apache License 2.0\nData / other materials: Creative Commons Attribution 4.0 (CC-BY 4.0)\nPermissibility:\nAcademic & research use:  Allowed\nCommercial use:  Allowed (with conditions)\nModification: Allowed\nRedistribution:  Allowed\nObligations\nApache-2.0: Preserve license and copyright notices\nCC-BY-4.0: Attribution required when redistributing or publishing derived work\nImportant nuance\nOpen X-Embodiment is an aggregate benchmark. Developers must check individual licenses associated with each component datasets"
        }
      ]
    },
    {
      "name": "LIBERO",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "The dataset contains 130 standardized robot manipulation tasks with human-teleoperated demonstration data across four task suites (LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-10) designed for lifelong learning research, featuring multi-view observations, robot end-effector state information, and actions for Franka Panda robotic arm.\n\n\nThe dataset is used for Objective O3 (ContextFlow) - In-context robot foundation model for evaluation and benchmarking on standardized manipulation tasks (130 tasks from LIBERO) to assess the model's ability to generalize to unseen tasks, as specified in the proposal's datasets section and methodology.\n\nAligns with the approved topic Datascience and Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The dataset is maintained by the Lifelong Robot Learning group, with contributions affiliated with Google DeepMind."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Inspection can be found in https://github.com/mshaikh786/aimcr-scripts/blob/master/65638-Elhoseiny/Dataset-inspection/libero/libero.pdf"
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Original creators: Bo Liu (University of Texas at Austin, now Meta), Yifeng Zhu (UT Austin), Chongkai Gao (National University of Singapore), Yihao Feng (Apple; UT Austin), Qiang Liu (UT Austin), Yuke Zhu (UT Austin, NVIDIA Research), Peter Stone (UT Austin)\nOriginal affiliation: University of Texas at Austin Learning Agents Research Group\n\nHugging Face dataset page:\nhttps://huggingface.co/datasets/physical-intelligence/libero\n\nDownloads (last month): 25,029\n\nGitHub repository:\nhttps://github.com/Lifelong-Robot-Learning/LIBERO/tree/master\n\nLast commit: ~9 months ago\nMaintainer: Google DeepMind\nStatistics:\nStars: ~1.3k\nCommits: 41\nPull Requests: 9\nIssues: 69"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Hugging Face license: CC-BY-4.0\nGitHub repository license: MIT License\nPermissibility:\n- Permitted for academic and research use\n- Redistribution requires inclusion of the original license\n- Attribution to the original authors is required under CC-BY-4.0"
        }
      ]
    },
    {
      "name": "Llava-onevision-dataset",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "The dataset contains 3,938,627 multimodal instruction-following samples (255 GB) across 89 subsets spanning mathematics, science, document understanding, OCR, visual reasoning, and general vision-language tasks aggregated from 60+ existing datasets.\n\nThe dataset is used for Objective O2 (LongVU++) - Scalable Long Video-Language Understanding, to embed vision knowledge into the base language model for temporal understanding and video processing.\n\nAligns with approved research topic datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "The dataset is owned and maintained by lmms-lab team led by Bo Li (drluodian@gmail.com) and Chunyuan Li, with key contributors including Kaichen Zhang, Yuanhan Zhang (Nanyang Technological University), Renrui Zhang, Feng Li, Hao Zhang, Dong Guo, and Haotian Liu.\n\nlmms-lab (also known as EvolvingLMMs-Lab) is a non-profit research-oriented organization focused on developing Large Multimodal Models (LMMs) and multimodal intelligence.\n\nPrimary Affiliation:\n\nNanyang Technological University (NTU), Singapore\nLed by Prof. Ziwei Liu\n\nCore Team Members:\n\nBo Li (PhD student at NTU, lead researcher) - drluodian@gmail.com\nChunyuan Li (research advisor, formerly ByteDance Seed/Microsoft Research)\nYuanhan Zhang, Kaichen Zhang, Peiyuan Zhang, Fanyi Pu, and others\n\nResearch Focus:\n\nBuilding open-source multimodal models (LLaVA series: LLaVA-OneVision, LLaVA-NeXT, LLaVA-Video)\nDeveloping evaluation frameworks (LMMs-Eval for image/video/audio tasks)\nCreating high-quality multimodal datasets\n\nFunding:\n\nSingapore Ministry of Education (MOE AcRF Tier 2)\nRIE2020 Industry Alignment Fund\n\nThe entity is in D5 country."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Inspection notebook available in /ibex/project/c2320/AIMCR/dataset-check/lava-onevision-dataset\n\nThe dataset contains visuals of 3D rendered objects, Graphs of different model training logs,  math questions with visuals, and distribution maps of the US.  There is no indication of any sensitive data in the randomly inspected samples."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "The dataset was first created in August 2024 by the lmms-lab team (Bo Li, Kaichen Zhang, et al.) through aggregation, cleaning, and re-annotation of 60+ existing datasets using GPT-4/GPT-4V, and is currently maintained by the same lmms-lab team as part of the LLaVA-NeXT project."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License type: modified Apache 2.0\n\nPermissions:\n\nCommercial use, modification, distribution, patent use, and private use allowed under Apache 2.0\nDataset can be freely used for training AI models\n\nRestrictions:\n\nMust include Apache 2.0 license text and copyright notice\nMust state significant changes made to the dataset\nNo trademark use granted\nGPT-4 generated portions subject to OpenAI Usage Policy (cannot compete with OpenAI)\nDataset creators specify \"academic research and education purpose only\"\nIndividual source datasets may have additional license requirements requiring attribution"
        }
      ]
    },
    {
      "name": "LLaVA-Video-178K",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "The dataset contains 178,510 videos (0-3 minutes duration) with 1.3M instruction samples including 178K detailed captions, 960K open-ended QA items, and 196K multiple-choice QA items, sourced from 10 major video datasets covering general activities, ego-vision, and academic scenarios.\n\nThe dataset is used for Objective O2 (Scalable Long Video-Language Understanding: LongVU++) to embed the model with spatial and temporal knowledge for processing videos at 1 FPS with up to 3,000 frames.\n\n Aligns with research topic datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 3,
          "notes": "Ownership: lmms-lab (LMMs-Lab organization at Nanyang Technological University)\nPrimary creators: Yuanhan Zhang (ByteDance/NTU), Jinming Wu, Wei Li, Bo Li (NTU PhD student, drluodian@gmail.com), Zejun Ma, Ziwei Liu (NTU S-Lab), Chunyuan Li (ByteDance/co-senior author)\nAffiliations: ByteDance, NTU S-Lab (Nanyang Technological University Singapore), BUPT (Beijing University of Posts and Telecommunications)\n\nLMMs-lab (also known as EvolvingLMMs-Lab) is a non-profit research-oriented organization focused on developing Large Multimodal Models (LMMs) and multimodal intelligence.\n\nPrimary Affiliation:\n\nNanyang Technological University (NTU), Singapore\nLed by Prof. Ziwei Liu\n\nCore Team Members:\n\nBo Li (PhD student at NTU, lead researcher) - drluodian@gmail.com\nChunyuan Li (research advisor, formerly ByteDance Seed/Microsoft Research)\nYuanhan Zhang, Kaichen Zhang, Peiyuan Zhang, Fanyi Pu, and others\n\nResearch Focus:\n\nBuilding open-source multimodal models (LLaVA series: LLaVA-OneVision, LLaVA-NeXT, LLaVA-Video)\nDeveloping evaluation frameworks (LMMs-Eval for image/video/audio tasks)\nCreating high-quality multimodal datasets\n\nFunding:\n\nSingapore Ministry of Education (MOE AcRF Tier 2)\nRIE2020 Industry Alignment Fund\n\nThe entity is in D5 country."
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 2,
          "notes": "The dataset mixes multiple upstream sources. Only a subset of videos is physically hosted on Hugging Face. The rest are references to external academic datasets, redistributed as metadata-only pointers due to licensing constraints.\n"
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "First created: October 3, 2024 by Yuanhan Zhang et al. (7 co-authors) as part of the \"Video Instruction Tuning With Synthetic Data\" paper (arXiv:2410.02713)\nCurrent maintainer: lmms-lab organization with ZhangYuanhan as primary HuggingFace contributor (499 commits as of dataset release)\n\nHF downloads: 15,566\n"
        },
        {
          "name": "License / Permissions",
          "score": 2,
          "notes": "Released under Apache 2.0\nPermissibility:\n- Academic and research use\n- Model training for non-commercial research\n-  Redistribution of raw videos requires caution\n- Commercial / production use without provenance audit is not recommended\nImportant Licensing Note:\n- The dataset aggregates videos from multiple upstream sources with potentially different licenses.\n- Use of the dataset does not grant blanket rights over all included video content.\n\nTo be used only for research and not for redistribution"
        }
      ]
    },
    {
      "name": "CinePile",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "This dataset is required for Objective O2: Scalable Long Video-Language Understanding (LongVU++)."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of prohibited use terms in the documentation."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Tom Goldstein's Lab at University of Maryland, College Park (tomg-group-umd) - Non D5 entity"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "None provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Skipping because risk level is less than 3."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://huggingface.co/datasets/tomg-group-umd/cinepile\nlast commit: 1 year ago\ncommits: 23\nAccess method: Direct download via Hugging Face datasets library\nHF Downloads: 176"
        },
        {
          "name": "License / Permissions",
          "score": 2,
          "notes": "Creative Commons Attribution Non Commercial Share Alike 4.0 International: https://spdx.org/licenses/CC-BY-NC-SA-4.0\nPermitted use:\n- Academic research\n- Model training & evaluation\nRestrictions:\n- Subject to non-commercial or attribution requirements\n- Underlying movie content may carry additional constraints"
        }
      ]
    },
    {
      "name": "InfiniBench",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Relevance to O2 (LongVU++)"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of prohibited use terms in the documentation. The dataset consists of questions derived from TV shows (Castle, Friends, House, Big Bang Theory, etc.)"
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "PI's own dataset created by the group \nnon-D5 entity"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Skipping because risk level is less than 3."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://huggingface.co/datasets/Vision-CAIR/InfiniBench\nOwned by Vision-CAIR \u2014 the same KAUST research group led by Mohamed Elhoseiny (the PI of this proposal). This is an internally developed dataset from the proposing team."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "License: BSD 3-Clause\nPermissions :\n- Commercial use \u2014 May be used for commercial purposes\n- Modification \u2014 May be modified\n- Distribution \u2014 May be distributed\n- Private use \u2014 May be used privately\nConditions :\nLicense and copyright notice \u2014 A copy of the license and copyright notice must be included with the software\nNo trademark use \u2014 The license does not grant trademark rights\nRestrictions:\n- No liability \n- No warranty\nImportant Note on Underlying Content:\nWhile the InfiniBench annotations and code are BSD 3-Clause licensed, the underlying video content (TV shows like Castle, Friends, House, Big Bang Theory, and movies) remains copyrighted by their respective studios. The dataset provides:\n- Question-answer annotations\n- Paths/references to video files\n- Temporal grounding information\nUsers must obtain the original video content separately through legitimate means (e.g., TVQA dataset, MovieNet dataset), as the copyright for TV shows and movies belongs to the original content owners."
        }
      ]
    },
    {
      "name": "VideoChat-Flash-Training-Data",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "algins with use in O2: Scalable Long Video-Language Understanding (LongVU++). Aligns with approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of prohibited use terms in the documentation. The dataset consists of video sinppet with annotations for multimodal model training."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "Owned by Shanghai AI Laboratory : https://www.shlab.org.cn/\nIt is an entity from D5 country"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "On the landing page of the datasets in HuggingFace, videos are playable for inspection. These are random activities from daily life, sports tutorials, etc. No sensitive sample was witnessed."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "90 commits, last commit was on Jun 24, 2025.\n48125 downloads and 17500 followers as of Jan 2, 2025\nThe dataset is an artifact of a paper : \"https://arxiv.org/html/2501.00574v4\" with authors from Chinese institutions. "
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Released under Apache 2.0\nPermissions:\n- Commercial use -- May be used for commercial purposes\n- Modification -- May be modified freely\n- Distribution -- May be distributed\n- Patent use -- Provides an express grant of patent rights from contributors\n- Private use -- May be used privately\n- Sublicensing -- May grant sublicenses to others\n\nLimitations:\n- No trademark use \u2014 Does not grant trademark rights\n- No liability \u2014 Authors are not liable for damages\n- No warranty \u2014 Provided \"as is\" without warranty"
        }
      ]
    },
    {
      "name": "MovieChat-1K",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "algins with use in O2: Scalable Long Video-Language Understanding (LongVU++). Aligns with approved research topic Datascience and engineering"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "The dataset consists of annotated long videos of movie clips with questions and answers. The listed genres are all benign. No detailed documentation of the dataset is available."
        },
        {
          "name": "D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The dataset is an artifact of the paper https://arxiv.org/abs/2307.16449. \nAuthors are from a mix of Chinese and US universities and Microsoft Research Asia. Project lead is from U Washington.\n"
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Samples inspected on the dataset card in HF. 3 MP4 samples were downloaded and checked, they were all parts of movies. The annotation also did not have prohibited terms."
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "https://huggingface.co/datasets/Enxin/MovieChat-1K_train\nhttps://github.com/rese1f/MovieChat\nThe dataset is artifact of a paper and maintained in two places, GitHub and HF. The following provenance is from HF.\nAs of Jan 3, 2025\n- 1887 commits in total\n- latest commit in May 2024\n- 9333 downloads\n- sole contributor/maintainer: https://huggingface.co/Enxin , a masters student from Zhejiang University https://enxinsong.com/"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "The license is mentioned in Github repo of the dataset. The dataset is released under BSD3-clause. The HF doesn't have a license attached which is a risk.  In general, fundamental research is permissible under this license. "
        }
      ]
    },
    {
      "name": "Falcon-RefineWeb",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "The dataset contains 968 million English web pages (about 1 billion instances) totaling 2.8TB of clean text data, built through stringent filtering and large-scale deduplication of CommonCrawl dumps from 2008 to January/February 2023.\n\nThe dataset is used for Phase 1: Language Recovery (RefinedWeb Edu) in the proposal's training pipeline, involving rapid continued pre-training to align new MLA (Multi-Head Latent Attention) layers with frozen/thawed MLP weights using the high-quality RefinedWeb Edu.\n\nAligned with the approved research topic,  Datascience and engineering.\n"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7.\n"
        },
        {
          "name": "D5+M affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "Ownership: Technology Innovation Institute (TII), Abu Dhabi Government's Advanced Technology Research Council\nPrimary creators/authors: Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei (Director, AI Cross-Center Unit, TII), Julien Launay\nAffiliations: Technology Innovation Institute (9639 Masdar City, Abu Dhabi, United Arab Emirates), LightOn, LPENS (\u00c9cole normale sup\u00e9rieure)\nContact: falconllm@tii.ae\n\nThe entity is from non-D5 country. "
        },
        {
          "name": "Prompts / Fine-tuning Scripts",
          "score": 1,
          "notes": "No user files provided."
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "Inspected on HF page. Text description with source URLs and corresponding images with URLs directed to them constitute the dataset "
        },
        {
          "name": "Provenance",
          "score": 1,
          "notes": "Based on CommonCrawl this is an artifact of the paper: https://arxiv.org/abs/2306.01116\nContact information provided on the dataset card: falconllm@tii.ae\nDownloads as of Jan 3, 2026: 55483\nMultiple commits were squashed into 1 commit in June 2023 which is the latest commit.\n\n"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": " Open Data Commons License\nPermission to use in fundamental research. "
        }
      ]
    }
  ],
  "models": [
    {
      "name": "TripoSG ",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "TripoSG is a Diffusion Transformer (DiT) model used for 3D shape generation (serving as the pre-trained initializer for the project\u2019s 3D asset generation pipeline). The proposal uses TripoSG to initialize the 3D asset generation component under O1 (simulation-ready 3D content generation). In the additional information, model name mentioned is CoMPoSE"
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7. "
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "URL: https://huggingface.co/VAST-AI/TripoSG - 692 Downloads Last Month\n\nThe model developers/maintainers are listed as VAST-AI (base model artifact: TripoSG on Hugging Face), based in USA.\nNo D5+M affiliation found."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "The model is listed as MIT License. \n\nPermits commercial use, modification, and redistribution.\n\nRequires preserving copyright/license notices in redistributed copies.\n\nProvided \u201cas is\u201d (warranty/liability limitations apply)."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "Pretraining data is Objaverse and Objaverse-XL, and the provided dataset-size proxy for compute estimation is ~3M samples in FP32."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Compositional 3D Shape Generation Fine-tuned on a custom dataset extracted from Objaverse. No mention of fine-tuning related to prohibited domains."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical training FLOPs (6\u00b7N\u00b7D)\nTheoretical FLOPs = 6 \u00d7 1.5\u00d710\u2079 \u00d7 3\u00d710\u2076 = 2.7\u00d710\u00b9\u2076 FLOPs.\nWe can treat each sample as T=2048 tokens (as used in the O1 DiT scaling discussion), then D \u2248 3M\u00d72048 = 6.144\u00d710\u2079 tokens and 6\u00b7N\u00b7D \u2248 5.53\u00d710\u00b9\u2079 FLOPs.\n\nDelivered FLOPs (GPU-hours \u00d7 peak \u00d7 MFU) (assuming a conservative MFU of 0.3)\n= 80,640 * 312 * 0.3 = 9.0574848\u00d710^22\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 and 2.5 checks. Skipping inspection. "
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Siglip2",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "SigLIP2 is a pretrained vision(-language) encoder used to turn images/videos into embeddings that can be projected into the LLM space for multimodal training.  \nIt is used in (O2) Scalable Long Video-Language Understanding as the visual encoder backbone for image/video inputs in the multimodal long-context pipeline. \n\nAligns with approved research topic of Datascience & Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7. "
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "HF URL: https://huggingface.co/google/siglip2-so400m-patch14-384\n285K Downloads last month\nOwned by Google\nNo D5+M affiliation found."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache 2.0 license is indicated for SigLIP2 in the provided documentation. "
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The documentation cites WebLI pretraining (10B images + 12B alt-texts, 109 languages) with an estimated ~14.5T token-equivalents usable as D. "
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "The provided documentation states no downstream fine-tuning was done for the released artifact (they use the pretrained model directly). "
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical training FLOPs (6\u00b7N\u00b7D)\nTheoretical FLOPs = 6 \u00b7 (4.0\u00d710^8) \u00b7 (1.45\u00d710^13) = 3.48\u00d710^22 FLOPs\n\n\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 and 2.5 checks. Skipping inspection. "
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Pi0",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Pi0 (Pi-Zero) is a Vision-Language-Action (VLA) model (PaliGemma backbone + an action expert trained with flow matching) that maps vision + instructions to robot actions for embodied/robot learning experiments. \nPi0/openpi is used for Objective (O3) \u201cContextFlow: In-context robot foundation model\u201d (pre-training + imitation fine-tuning / embodied AI).\n\nAligns with approved research topic of Datascience & Engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7. "
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "GitHub URL: https://github.com/Physical-Intelligence/openpi \n10K stars - 190 Commits - Last updated one month ago\nOwned By: Physical Intelligence \nLinkedIn: https://www.linkedin.com/company/physical-intelligence\nUSA\nNo D5+M affiliation found"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0.\n\nYou may use, modify, and redistribute the code (source/binary).\n\nYou must include the license and preserve required copyright/NOTICE attributions.\n\nYou must state significant changes you made (when distributing).\n\nThe license includes a patent grant (with termination conditions in case of patent litigation).\n\nNo trademark rights are granted beyond descriptive use."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The openpi base checkpoints are described as pre-trained on \u201c10k+ hours of robot data,\u201d while the proposal\u2019s O3 training plan uses Open-X (~1,000,000 trajectories) plus a similarly sized simulation dataset and samples ~1.95M trajectories for compute estimation (we can use this as D in \u201ctoken-equivalents\u201d if you treat 1 trajectory \u2248 1 unit)."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "The repository provides \u201cexpert\u201d fine-tuned checkpoints for specific robot platforms/tasks (the docs explicitly mention expert checkpoints and training instructions for DROID, and reference platforms like ALOHA)."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "3.3B parameters (\u22483B VLM + \u2248300M action expert). \nFor Objective O3 (ContextFlow), the proposal allocates ~100K GH200 GPU-hours total (\u224897.5K pretraining + \u22482.5K fine-tuning). \n\nTheoretical training FLOPs (6\u00b7N\u00b7D)\nTheoretical FLOPs = 6\u00d73.3\u00d710^9\u00d750\u00d710^9=9.9\u00d710^20 FLOPs\n\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 and 2.5 checks. Skipping inspection. "
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen3-VL-8B-Instruct ",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Qwen3-VL-8B-Instruct is a vision-language model that takes images/video + text and generates text responses, supporting long-context (native 256K, expandable) multimodal understanding and agentic visual interaction.\nIt best aligns with [O2] Scalable Long Video-Language Understanding, which targets large-scale fine-tuning for multimodal long-video modeling and long-context scaling via sequence-parallel methods. \nIt is listed in additional information under the name \"Think With Video\", where it is indicated for doing full fine-tuning on 3M samples from Objaverse and Objaverse XL datasets. \n\nAligns with the approved research topic of Datascience & Engineering. "
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "The model is maintained by the Qwen Team (Alibaba Cloud / Alibaba Group)  \nCountry of origin: China\nD5-Country affiliation found."
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 licensed (per model card and repo).\n- In the additional info, MIT license is mentioned.\n\nPermits commercial use, modification, and redistribution with required license/NOTICE preservation and attribution.\n\nProvides an express patent license from contributors; terminates for certain patent claims."
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The Qwen3 technical report states pretraining uses ~36 trillion tokens (used here as the token-equivalent D estimate, noting Qwen3-VL is multimodal so the true multimodal \u201ctoken-equivalent\u201d may differ)."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "Video question answering Instruct finetuning LLaVA-Video-178K "
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical compute: F \u2248 6 \u00b7 N \u00b7 D\n\nN = 9B params\n\nD \u2248 36T tokens\n\nEstimated FLOPS \u2248 6 \u00d7 9e9 \u00d7 36e12 = 1.944 \u00d7 10\u00b2\u2074 FLOPs\n"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-7B",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Qwen 2.5 7B is a multimodal large language model designed for natural language understanding, text generation, coding, mathematics, and multilingual support.\n\nThis model has been provided with additional information by the PI. It has been indicated for Instruct finetuning on LLaVA-Video-178K.\n\n\nAligns with the approved research project of Datascience & Engineering.  "
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\n\nCountry of origin: China D5-Country affiliation found.\n\nHuggingFace: https://huggingface.co/Qwen/Qwen2.5-7B\nPaper: https://arxiv.org/abs/2407.10671"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\n\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially\nUsers must provide a copy of the Apache 2.0 license with distributions\nUsers must document any modifications made\nUsers may add their own copyright statements to modifications\nPatent licenses terminate if patent litigation is filed against the Work\nNo trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical compute: F \u2248 6 \u00b7 N \u00b7 D\nN = 7B params\n\nD \u2248 18T tokens\n\nEstimaeted FLOPS= 6*7*10^9*18*10^12 = 7.56 * 10^23"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-7B-Instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": " Qwen2.5 instruct is a general-purpose large language model for text generation, instruction following, long-text generation, structured data analysis, coding, mathematics, and multilingual conversations across 29+ languages.\n\nIndicated in additional information for RL-based agentic search experiments.\n\nAligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 2,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\n\nCountry of origin: China D5-Country affiliation found.\n\nHuggingFace: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Qwen License (custom, based on Tongyi Qianwen License):\nFree to use, reproduce, modify, and distribute for commercial and non-commercial purposes \nMust include copy of license and attribution notices with distributions \nModified files must carry prominent modification notices If commercial use exceeds 100 million monthly active users, must request additional license from Alibaba \nCannot use model outputs to improve other LLMs (except Qwen derivatives) \nMust comply with export controls and applicable laws \nYou own derivative works you create \nNo trademark license granted except for required notices"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "Theoretical compute: F \u2248 6 \u00b7 N \u00b7 D\nN = 7B params\n\nD \u2248 18T tokens\n\nEstimated FLOPS = 6*7*10^9*18*10^12 = 7.56 * 10^23"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-14B",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Large language model designed for natural language understanding, text generation, coding, mathematics, multilingual support."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-14B\nPaper: https://arxiv.org/abs/2407.10671"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 14B params\nD \u2248 18T tokens\n\nF= 6 \u00b714 \u00b710^9 \u00b718 \u00b710^12 = 1.512\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-14B-Instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": " Qwen2.5 instruct is a general-purpose large language model for text generation, instruction following, long-text generation, structured data analysis, coding, mathematics, and multilingual conversations across 29+ languages. Aligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-14B-Instruct"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 14B params\nD \u2248 18T tokens\n\nF= 6 \u00b714 \u00b710^9 \u00b718 \u00b710^12 = 1.512\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-32B",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": " Large language model designed for natural language understanding, text generation, coding, mathematics, multilingual support."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-32B"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 32B params\nD \u2248 18T tokens\n\nF= 6 \u00b732 \u00b710^9 \u00b718 \u00b710^12 = 3.456\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    },
    {
      "name": "Qwen 2.5-32B-Instruct",
      "checks": [
        {
          "name": "Project & Usage Alignment",
          "score": 1,
          "notes": "Qwen2.5 instruct is a general-purpose large language model for text generation, instruction following, long-text generation, structured data analysis, coding, mathematics, and multilingual conversations across 29+ languages. Aligns with approved research topic Datascience and engineering."
        },
        {
          "name": "Prohibited Use Screening (LC 2.7)",
          "score": 1,
          "notes": "No mention of sensitive terms in the documentation classified in Prohibited Use Screening in accordance with LC 2.7."
        },
        {
          "name": "Source / Provenance & D5+M Affiliation Screening (LC 2.5)",
          "score": 1,
          "notes": "The model is owned and maintained by the Qwen Team at Alibaba Group (Alibaba Cloud); LinkedIn profiles found include Junyang Lin (Tech Lead, Qwen Team) at https://www.linkedin.com/in/junyang-lin-0b2b38151/ and Tianhang Zhu (Core Member) at https://www.linkedin.com/in/bobzhu/.\nCountry of origin: China D5-Country affiliation found.\n\nSource: https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"
        },
        {
          "name": "License / Permissions",
          "score": 1,
          "notes": "Apache-2.0 license\nUsers may freely use, reproduce, modify, and distribute the model commercially or non-commercially Users must provide a copy of the Apache 2.0 license with distributions Users must document any modifications made Users may add their own copyright statements to modifications Patent licenses terminate if patent litigation is filed against the Work No trademark rights are granted beyond reasonable use in describing origin"
        },
        {
          "name": "Training Data Documentation",
          "score": 1,
          "notes": "The model was pretrained on up to 18 trillion tokens from multilingual datasets (approximately 30 languages); estimated D term for Chinchilla scaling: 18 \u00d7 10^12 tokens."
        },
        {
          "name": "Customisation / Fine-tuning",
          "score": 1,
          "notes": "This model, as is, has not been fine-tuned on any downstream task, according to the model card from HuggingFace."
        },
        {
          "name": "FLOPS Calculation",
          "score": 1,
          "notes": "F \u2248 6 \u00b7 N \u00b7 D \nN = 32B params\nD \u2248 18T tokens\n\nF= 6 \u00b732 \u00b710^9 \u00b718 \u00b710^12 = 3.456\u00b710^24"
        },
        {
          "name": "Sample Inspection",
          "score": 1,
          "notes": "No risk in LC2.7 check. Skipping inspection."
        }
      ],
      "is_proprietary": false
    }
  ],
  "observations": "\n- \nIn software, ROLL and RLinf libraries have maintainers from D5 countries. Some dependencies were being pulled from Alibaba cloud registries but KSL will recreate the container images with same libraries pulled from Pypi.org.\n \nThe datasets RoboMIND, Llava-onevision-dataset are from an entity in the D5 country.  Sample inspect of the datasets was conducted and the risk has been set to low because the content of the dataset, randomly checked, corresponds to its description.\n\n  The Llava-Video-178K datasets is also maintained by the same entity in D5 country. The dataset is a collection of different external datasets not readily accessible therefore, the risk has been raised to medium. \n\n\nMultiple Qwen 2.5 and 3 models have been requested for supervised fine-tuning and reinforcement learning with human in the loop (RLHF) on Video dataset like Llava-video-178k. All the models are from D5 country, developed and maintained by Alibaba cloud. ",
  "recommendation": "Recommended for grand challenge conditional to acceptable risk with models. "
}