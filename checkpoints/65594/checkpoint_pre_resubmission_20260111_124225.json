{
  "checkpoint_metadata": {
    "type": "pre_resubmission",
    "timestamp": "2026-01-11T12:42:25.717667",
    "project_id": "65594"
  },
  "form_data": {
    "metadata": {
      "proposal_title": "Scaling Laws and Optimization Dynamics of Looped Transformers: Toward Efficient and High-Capacity Language Models",
      "principal_investigator": "Di Wang",
      "proposal_date": "2026-01-11",
      "reviewer_name": "Mohsin Ahmed Shaikh",
      "reviewer_id": "174988",
      "aimcr_date": "2026-01-11",
      "project_id": "65594"
    },
    "third_party_software": [
      {
        "name": "PyTorch",
        "checks": [
          {
            "name": "Project & Usage Alignment",
            "score": 1,
            "notes": "Required to implement and run transformer-based LLM training experiments (including distributed training with Megatron-LM) to study Looped Transformer scaling laws and optimization dynamics. Aligns with the approved topic, Data Science and Engineering."
          },
          {
            "name": "Prohibited Use Screening (LC 2.7)",
            "score": 1,
            "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the PyTorch 2.9.0 documentation, which describes a general-purpose open-source machine learning framework; this review was conducted in accordance with LC 2.7."
          },
          {
            "name": "Restricted Entities Screening (LC 2.5)",
            "score": 1,
            "notes": "No indication of restricted-country origin or restricted-entity involvement was identified for Torchtitan v0.2.0 based on available documentation, contributor information, and repository metadata.\\nThis review was conducted in accordance with LC 2.5."
          },
          {
            "name": "Source / Provenance",
            "score": 1,
            "notes": "3.1 Source Channel\nGitHub: https://github.com/pytorch/pytorch\n3.2 GitHub Repository Metadata\nOwner / Organization: PyTorch Foundation\nRepository: pytorch/pytorch\nDescription: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nStars: 96510\nForks: 26483\nOpen Issues: 17875\nLast Commit Date: 2026-01-11T03:50:11Z\nActivity Health: Healthy"
          },
          {
            "name": "License / Permissions",
            "score": 1,
            "notes": "4.1 License Type\nSPDX ID: N/A\nLicense name: BSD 3-Clause\n4.2 Permissibility Summary\nQuestion\tAnswer\nCan be used for academic purposes?\tYes\nCan be used for commercial use?\tYes\nModification allowed?\tYes\nRedistribution allowed?\tYes\nAttribution required?\tYes\nCopyleft obligations?\tNo"
          },
          {
            "name": "Bundled Tools / Dependencies",
            "score": 1,
            "notes": "torch 2.9.0\nnvidia-cublas-cu12 12.8.4.1\nnvidia-cuda-cupti-cu12 12.8.90\nnvidia-cuda-nvrtc-cu12 12.8.93\nnvidia-cuda-runtime-cu12 12.8.90\nnvidia-cudnn-cu12 9.10.2.21\nnvidia-cufft-cu12 11.3.3.83\nnvidia-cufile-cu12 1.13.1.3\nnvidia-curand-cu12 10.3.9.90\nnvidia-cusolver-cu12 11.7.3.90\nnvidia-cusparse-cu12 12.5.8.93\nnvidia-cusparselt-cu12 0.7.1\nnvidia-nccl-cu12 2.27.5\nnvidia-nvjitlink-cu12 12.8.93\nnvidia-nvshmem-cu12 3.3.20\nnvidia-nvtx-cu12 12.8.90\ntriton 3.5.0\nfsspec 2026.1.0\nnetworkx 3.6.1\nsympy 1.14.0\nmpmath 1.3.0\ntyping_extensions 4.15.0\nfilelock 3.20.3\nJinja2 3.1.6\nMarkupSafe 3.0.3\nsetuptools 80.9.0\n"
          }
        ]
      },
      {
        "name": "Megatron-LM (0.15.0)",
        "checks": [
          {
            "name": "Project & Usage Alignment",
            "score": 1,
            "notes": "Required to implement and scale large transformer model pretraining to support the project\u2019s objectives of analyzing and optimizing LLM training dynamics. Aligns with the approved topic, Data Science and Engineering."
          },
          {
            "name": "Prohibited Use Screening (LC 2.7)",
            "score": 1,
            "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the Megatron-LM v0.15.0 documentation, which describes a research-oriented large-scale language model training framework; this review was conducted in accordance with LC 2.7."
          },
          {
            "name": "Restricted Entities Screening (LC 2.5)",
            "score": 1,
            "notes": "No indication of restricted-country origin or restricted-entity involvement was identified for Torchtitan v0.2.0 based on available documentation, contributor information, and repository metadata. This review was conducted in accordance with LC 2.5."
          },
          {
            "name": "Source / Provenance",
            "score": 1,
            "notes": "3.1 Source Channel\nGitHub: https://github.com/NVIDIA/Megatron-LM\n3.2 GitHub Repository Metadata\nOwner / Organization: NVIDIA\nRepository: NVIDIA/Megatron-LM\nDescription: Ongoing research training transformer models at scale\nStars: 14862\nForks: 3478\nOpen Issues: 558\nLast Commit Date: 2026-01-10T07:07:09Z\nActivity Health: Healthy"
          },
          {
            "name": "License / Permissions",
            "score": 1,
            "notes": ".1 License Type\nSPDX ID: N/A\nLicense name: Apache 2.0\n4.2 Permissibility Summary\nQuestion\tAnswer\nCan be used for academic purposes?\tYes\nCan be used for commercial use?\tYes\nModification allowed?\tYes\nRedistribution allowed?\tYes\nAttribution required?\tYes\nCopyleft obligations?\tNo"
          },
          {
            "name": "Bundled Tools / Dependencies",
            "score": 1,
            "notes": "megatron-core 0.15.0\nnumpy 1.26.4\npackaging 25.0\ntorch 2.9.1\nnvidia-cublas-cu12 12.8.4.1\nnvidia-cuda-cupti-cu12 12.8.90\nnvidia-cuda-nvrtc-cu12 12.8.93\nnvidia-cuda-runtime-cu12 12.8.90\nnvidia-cudnn-cu12 9.10.2.21\nnvidia-cufft-cu12 11.3.3.83\nnvidia-cufile-cu12 1.13.1.3\nnvidia-curand-cu12 10.3.9.90\nnvidia-cusolver-cu12 11.7.3.90\nnvidia-cusparse-cu12 12.5.8.93\nnvidia-cusparselt-cu12 0.7.1\nnvidia-nccl-cu12 2.27.5\nnvidia-nvjitlink-cu12 12.8.93\nnvidia-nvshmem-cu12 3.3.20\nnvidia-nvtx-cu12 12.8.90\ntriton 3.5.1\nfsspec 2026.1.0\nnetworkx 3.6.1\nsympy 1.14.0\nmpmath 1.3.0\ntyping_extensions 4.15.0\nfilelock 3.20.3\nJinja2 3.1.6\nMarkupSafe 3.0.3\nsetuptools 80.9.0\n"
          }
        ]
      }
    ],
    "source_code": [],
    "datasets_user_files": [
      {
        "name": "FineWeb-Edu",
        "checks": [
          {
            "name": "Project & Usage Alignment",
            "score": 1,
            "notes": "It's the only mentioned dataset."
          },
          {
            "name": "Prohibited Use Screening (LC 2.7)",
            "score": 1,
            "notes": "No indication of prohibited or restricted use (including military, weapons, surveillance, or intelligence functionality) is stated or implied in the FineWeb-Edu dataset documentation, which describes the dataset as an educationally focused web-text corpus for language model research and training; this review was conducted in accordance with LC 2.7."
          },
          {
            "name": "Restricted Entities Screening (LC 2.5)",
            "score": 1,
            "notes": "Originating Organization: HuggingFace and Common Crawl both in United States\n"
          },
          {
            "name": "Prompts / Fine-tuning Scripts",
            "score": 1,
            "notes": "No files available"
          },
          {
            "name": "Sample Inspection",
            "score": 1,
            "notes": "Preview available by HuggingFace\nhttps://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/viewer/default/train?row=0"
          },
          {
            "name": "Provenance",
            "score": 1,
            "notes": "\nOwner / Curator: HuggingFace / Fine Data\n\nPrimary Distribution Platform: Hugging Face\n\nRepository: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n\nAccess Method: download via Hugging Face\n\nProvenance:\n\nHugging Face does not claim ownership of the original webpages\nPrimary raw source: Common Crawl\nHugging Face does own and license the compiled dataset artifact i.e., the database resulting from selection, transformation, and organization of the data\n(277k downloads on HF, 1 commit, July 2025).\nMaintenance Status: Actively maintained (~20 commits; last update 2025-12)"
          },
          {
            "name": "License / Permissions",
            "score": 1,
            "notes": "License: ODC-BY\nUse the dataset for any purpose (research, commercial, internal, public)\nModify, transform, and build upon the dataset\nAttribution to the dataset authors is required\nIndicate if changes were made"
          }
        ]
      }
    ],
    "models": [],
    "observations": "No concrete, named pretrained models or exact model variants are specified (e.g., \u201cLLaMA-2-7B\u201d, \u201cQwen-7B\u201d, \u201cGPT-NeoX-20B\u201d).\n\nFinding\n- The document does not name any specific existing model checkpoints or fixed model sizes to be used.\n- References to \u201cLlama/Qwen style\u201d are architectural examples only, not commitments to specific models.\n- The project instead proposes training new Transformer-based and Looped Transformer models from scratch across a range of sizes and loop depths. \n",
    "recommendation": "Since the proposal is for a looped transformer, the expectation is that the number of parameters will be significantly less then any stacked transformer architecture. However, the number of FLOPs will remain undetermined due to the lack of information about the architecture. \nEven with this gap, the project can progress to grand challenge with requirement of consultation with the applicants and monitoring when in use.\n\nRecommended for grand challenge. "
  }
}